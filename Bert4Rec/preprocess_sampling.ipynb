{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".venv",
   "display_name": ".venv",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from abc import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "from abc import *\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Class that will hold our parameter values as attributes, thus making it easy to call\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.random_seed = 10\n",
    "        \n",
    "        #Data Split\n",
    "        self.train_size = 0.8\n",
    "        self.test_size = 1- self.train_size\n",
    "        self.save_folder = '../data/bert_4_rec_data'\n",
    "        \n",
    "        #Since we are doing collaborative filtering, we are only interested in users and movies with significant overlaps\n",
    "        self.min_movie_overlap = 5\n",
    "        self.min_user_overlap = 5\n",
    "\n",
    "        #Sampling\n",
    "        self.train_negative_sampler_code = 'random'\n",
    "        self.train_negative_sample_size = 0\n",
    "        self.train_negative_sampling_seed = 0\n",
    "        self.test_negative_sampler_code = 'random'\n",
    "        self.test_negative_sample_size = 100\n",
    "        self.test_negative_sampling_seed = 98765\n",
    "\n",
    "        #BERT Model\n",
    "        self.bert_max_len = 100\n",
    "        self.bert_mask_prob = 0.15\n",
    "        self.batch_size = 128\n",
    "\n",
    "\n",
    "\n",
    "args = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset = pd.read_pickle('../data/bert_4_rec_data/dataset.pkl')\n",
    "\n",
    "except:\n",
    "    data = pd.read_excel('../data/processed_data/full_data.xlsx')\n",
    "    data = data.drop(columns = \"Unnamed: 0\")\n",
    "\n",
    "    ## Retriving the relevant features\n",
    "    ratings = pd.DataFrame()\n",
    "\n",
    "    ratings['userId'] = data['userId']\n",
    "    ratings['movieId'] = data['movieId']\n",
    "    ratings['rating'] = data['rating']\n",
    "    ratings['timestamp'] = data['timestamp_ratings']\n",
    "\n",
    "    # Since we are considering good movies we will filter out those rating below 3.5 (Determined by the histogram of ratings)\n",
    "    ratings = ratings[ratings['rating'] >= 3.5]\n",
    "\n",
    "    # Filter only user and movies with significant overlaps\n",
    "\n",
    "    item_sizes = ratings.groupby('movieId').size()\n",
    "    good_items = item_sizes.index[item_sizes >= args.min_movie_overlap]\n",
    "    ratings = ratings[ratings['movieId'].isin(good_items)]\n",
    "\n",
    "\n",
    "    user_sizes = ratings.groupby('userId').size()\n",
    "    good_users = user_sizes.index[user_sizes >= args.min_user_overlap]\n",
    "    ratings = ratings[ratings['userId'].isin(good_users)]\n",
    "\n",
    "    # Probability Density  Distribution of user and movie\n",
    "\n",
    "    print('Densifying index')\n",
    "    umap = {u: i for i, u in enumerate(set(ratings['userId']))}\n",
    "    smap = {s: i for i, s in enumerate(set(ratings['movieId']))}\n",
    "    ratings['userId'] = ratings['userId'].map(umap)\n",
    "    ratings['movieId'] = ratings['movieId'].map(smap)\n",
    "\n",
    "    # Since we are considering the changing user preference overtime, we have to model the data as a timeseries and split train/test by specifying a specific timepoint to split the data (Hold-Out Sampling)\n",
    "\n",
    "    np.random.seed(args.random_seed)\n",
    "    user_count = len(ratings.userId.unique())\n",
    "\n",
    "    # Generate user indices\n",
    "    permuted_index = np.random.permutation(user_count)\n",
    "    eval_set_size = int(len(permuted_index)*args.test_size)\n",
    "\n",
    "    train_user_index = permuted_index[                :-2*eval_set_size]\n",
    "    val_user_index   = permuted_index[-2*eval_set_size:  -eval_set_size]\n",
    "    test_user_index  = permuted_index[  -eval_set_size:                ]\n",
    "\n",
    "    # Split DataFrames\n",
    "    train_df = ratings.loc[ratings['userId'].isin(train_user_index)]\n",
    "    val_df   = ratings.loc[ratings['userId'].isin(val_user_index)]\n",
    "    test_df  = ratings.loc[ratings['userId'].isin(test_user_index)]\n",
    "\n",
    "    # DataFrame to dict => {uid : list of sid's}\n",
    "    train = dict(train_df.groupby('userId').apply(lambda d: list(d['movieId'])))\n",
    "    val   = dict(val_df.groupby('userId').apply(lambda d: list(d['movieId'])))\n",
    "    test  = dict(test_df.groupby('userId').apply(lambda d: list(d['movieId'])))\n",
    "\n",
    "    dataset = {'train': train,\n",
    "                'val': val,\n",
    "                'test': test,\n",
    "                'umap': umap,\n",
    "                'smap': smap}\n",
    "\n",
    "    with open('../data/bert_4_rec_data/dataset.pkl', \"wb\") as e:\n",
    "        pickle.dump(dataset, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   userId  movieId  rating  timestamp_ratings               tag  \\\n",
       "0       4     7569     3.5         1573943431  so bad it's good   \n",
       "1   18853     7569     3.0         1248068143               007   \n",
       "2   18853     7569     3.0         1248068143        james bond   \n",
       "3   21096     7569     4.0         1269243364         franchise   \n",
       "4   21096     7569     4.0         1269243364        James Bond   \n",
       "\n",
       "   timestamp_tags                title                          genres_x  \\\n",
       "0      1573943455  You Only Live Twice  Action|Adventure|Sci-Fi|Thriller   \n",
       "1      1248068150  You Only Live Twice  Action|Adventure|Sci-Fi|Thriller   \n",
       "2      1248068148  You Only Live Twice  Action|Adventure|Sci-Fi|Thriller   \n",
       "3      1246471298  You Only Live Twice  Action|Adventure|Sci-Fi|Thriller   \n",
       "4      1246471305  You Only Live Twice  Action|Adventure|Sci-Fi|Thriller   \n",
       "\n",
       "   MovieYear     tconst  ...        originalTitle isAdult startYear  endYear  \\\n",
       "0       1977  tt0062512  ...  You Only Live Twice       0      1967       \\N   \n",
       "1       1977  tt0062512  ...  You Only Live Twice       0      1967       \\N   \n",
       "2       1977  tt0062512  ...  You Only Live Twice       0      1967       \\N   \n",
       "3       1977  tt0062512  ...  You Only Live Twice       0      1967       \\N   \n",
       "4       1977  tt0062512  ...  You Only Live Twice       0      1967       \\N   \n",
       "\n",
       "  runtimeMinutes                   genres_y averageRating numVotes  directors  \\\n",
       "0            117  Action,Adventure,Thriller           6.9    99691  nm0318150   \n",
       "1            117  Action,Adventure,Thriller           6.9    99691  nm0318150   \n",
       "2            117  Action,Adventure,Thriller           6.9    99691  nm0318150   \n",
       "3            117  Action,Adventure,Thriller           6.9    99691  nm0318150   \n",
       "4            117  Action,Adventure,Thriller           6.9    99691  nm0318150   \n",
       "\n",
       "                                   writers  \n",
       "0  nm0089169,nm0001094,nm0001220,nm0420845  \n",
       "1  nm0089169,nm0001094,nm0001220,nm0420845  \n",
       "2  nm0089169,nm0001094,nm0001220,nm0420845  \n",
       "3  nm0089169,nm0001094,nm0001220,nm0420845  \n",
       "4  nm0089169,nm0001094,nm0001220,nm0420845  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp_ratings</th>\n      <th>tag</th>\n      <th>timestamp_tags</th>\n      <th>title</th>\n      <th>genres_x</th>\n      <th>MovieYear</th>\n      <th>tconst</th>\n      <th>...</th>\n      <th>originalTitle</th>\n      <th>isAdult</th>\n      <th>startYear</th>\n      <th>endYear</th>\n      <th>runtimeMinutes</th>\n      <th>genres_y</th>\n      <th>averageRating</th>\n      <th>numVotes</th>\n      <th>directors</th>\n      <th>writers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>7569</td>\n      <td>3.5</td>\n      <td>1573943431</td>\n      <td>so bad it's good</td>\n      <td>1573943455</td>\n      <td>You Only Live Twice</td>\n      <td>Action|Adventure|Sci-Fi|Thriller</td>\n      <td>1977</td>\n      <td>tt0062512</td>\n      <td>...</td>\n      <td>You Only Live Twice</td>\n      <td>0</td>\n      <td>1967</td>\n      <td>\\N</td>\n      <td>117</td>\n      <td>Action,Adventure,Thriller</td>\n      <td>6.9</td>\n      <td>99691</td>\n      <td>nm0318150</td>\n      <td>nm0089169,nm0001094,nm0001220,nm0420845</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18853</td>\n      <td>7569</td>\n      <td>3.0</td>\n      <td>1248068143</td>\n      <td>007</td>\n      <td>1248068150</td>\n      <td>You Only Live Twice</td>\n      <td>Action|Adventure|Sci-Fi|Thriller</td>\n      <td>1977</td>\n      <td>tt0062512</td>\n      <td>...</td>\n      <td>You Only Live Twice</td>\n      <td>0</td>\n      <td>1967</td>\n      <td>\\N</td>\n      <td>117</td>\n      <td>Action,Adventure,Thriller</td>\n      <td>6.9</td>\n      <td>99691</td>\n      <td>nm0318150</td>\n      <td>nm0089169,nm0001094,nm0001220,nm0420845</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18853</td>\n      <td>7569</td>\n      <td>3.0</td>\n      <td>1248068143</td>\n      <td>james bond</td>\n      <td>1248068148</td>\n      <td>You Only Live Twice</td>\n      <td>Action|Adventure|Sci-Fi|Thriller</td>\n      <td>1977</td>\n      <td>tt0062512</td>\n      <td>...</td>\n      <td>You Only Live Twice</td>\n      <td>0</td>\n      <td>1967</td>\n      <td>\\N</td>\n      <td>117</td>\n      <td>Action,Adventure,Thriller</td>\n      <td>6.9</td>\n      <td>99691</td>\n      <td>nm0318150</td>\n      <td>nm0089169,nm0001094,nm0001220,nm0420845</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21096</td>\n      <td>7569</td>\n      <td>4.0</td>\n      <td>1269243364</td>\n      <td>franchise</td>\n      <td>1246471298</td>\n      <td>You Only Live Twice</td>\n      <td>Action|Adventure|Sci-Fi|Thriller</td>\n      <td>1977</td>\n      <td>tt0062512</td>\n      <td>...</td>\n      <td>You Only Live Twice</td>\n      <td>0</td>\n      <td>1967</td>\n      <td>\\N</td>\n      <td>117</td>\n      <td>Action,Adventure,Thriller</td>\n      <td>6.9</td>\n      <td>99691</td>\n      <td>nm0318150</td>\n      <td>nm0089169,nm0001094,nm0001220,nm0420845</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21096</td>\n      <td>7569</td>\n      <td>4.0</td>\n      <td>1269243364</td>\n      <td>James Bond</td>\n      <td>1246471305</td>\n      <td>You Only Live Twice</td>\n      <td>Action|Adventure|Sci-Fi|Thriller</td>\n      <td>1977</td>\n      <td>tt0062512</td>\n      <td>...</td>\n      <td>You Only Live Twice</td>\n      <td>0</td>\n      <td>1967</td>\n      <td>\\N</td>\n      <td>117</td>\n      <td>Action,Adventure,Thriller</td>\n      <td>6.9</td>\n      <td>99691</td>\n      <td>nm0318150</td>\n      <td>nm0089169,nm0001094,nm0001220,nm0420845</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "data = pd.read_excel('../data/processed_data/full_data.xlsx')\n",
    "data = data.drop(columns = \"Unnamed: 0\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       4     7569     3.5  1573943431\n",
       "1   18853     7569     3.0  1248068143\n",
       "2   18853     7569     3.0  1248068143\n",
       "3   21096     7569     4.0  1269243364\n",
       "4   21096     7569     4.0  1269243364"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>7569</td>\n      <td>3.5</td>\n      <td>1573943431</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18853</td>\n      <td>7569</td>\n      <td>3.0</td>\n      <td>1248068143</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18853</td>\n      <td>7569</td>\n      <td>3.0</td>\n      <td>1248068143</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21096</td>\n      <td>7569</td>\n      <td>4.0</td>\n      <td>1269243364</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21096</td>\n      <td>7569</td>\n      <td>4.0</td>\n      <td>1269243364</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "## Retriving the relevant features\n",
    "ratings = pd.DataFrame()\n",
    "\n",
    "ratings['userId'] = data['userId']\n",
    "ratings['movieId'] = data['movieId']\n",
    "ratings['rating'] = data['rating']\n",
    "ratings['timestamp'] = data['timestamp_ratings']\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4.0    202717\n",
       "5.0    167506\n",
       "3.5    141415\n",
       "4.5    140241\n",
       "3.0     98314\n",
       "2.5     49452\n",
       "2.0     37708\n",
       "1.5     18866\n",
       "1.0     16465\n",
       "0.5     13742\n",
       "Name: rating, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "ratings.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only user and movies with significant overlaps\n",
    "\n",
    "item_sizes = ratings.groupby('movieId').size()\n",
    "good_items = item_sizes.index[item_sizes >= min_movie_overlap]\n",
    "ratings = ratings[ratings['movieId'].isin(good_items)]\n",
    "\n",
    "\n",
    "user_sizes = ratings.groupby('userId').size()\n",
    "good_users = user_sizes.index[user_sizes >= min_user_overlap]\n",
    "ratings = ratings[ratings['userId'].isin(good_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Densifying index\n"
     ]
    }
   ],
   "source": [
    "# Probability Density  Distribution of user and movie\n",
    "\n",
    "print('Densifying index')\n",
    "umap = {u: i for i, u in enumerate(set(ratings['userId']))}\n",
    "smap = {s: i for i, s in enumerate(set(ratings['movieId']))}\n",
    "ratings['userId'] = ratings['userId'].map(umap)\n",
    "ratings['movieId'] = ratings['movieId'].map(smap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrameGroupBy' object has no attribute 'progress_apply'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-623c4405c168>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# DataFrame to dict => {uid : list of sid's}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'userId'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movieId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mval\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'userId'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movieId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mtest\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'userId'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movieId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m    704\u001b[0m             \u001b[1;34mf\"'{type(self).__name__}' object has no attribute '{attr}'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrameGroupBy' object has no attribute 'progress_apply'"
     ]
    }
   ],
   "source": [
    "# Since we are considering the changing user preference overtime, we have to model the data as a timeseries and split train/test by specifying a specific timepoint to split the data (Hold-Out Sampling)\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "user_count = len(ratings.userId.unique())\n",
    "\n",
    "# Generate user indices\n",
    "permuted_index = np.random.permutation(user_count)\n",
    "eval_set_size = int(len(permuted_index)*args.test_size)\n",
    "\n",
    "train_user_index = permuted_index[                :-2*eval_set_size]\n",
    "val_user_index   = permuted_index[-2*eval_set_size:  -eval_set_size]\n",
    "test_user_index  = permuted_index[  -eval_set_size:                ]\n",
    "\n",
    "# Split DataFrames\n",
    "train_df = ratings.loc[ratings['userId'].isin(train_user_index)]\n",
    "val_df   = ratings.loc[ratings['userId'].isin(val_user_index)]\n",
    "test_df  = ratings.loc[ratings['userId'].isin(test_user_index)]\n",
    "\n",
    "# DataFrame to dict => {uid : list of sid's}\n",
    "train = dict(train_df.groupby('userId').apply(lambda d: list(d['movieId'])))\n",
    "val   = dict(val_df.groupby('userId').apply(lambda d: list(d['movieId'])))\n",
    "test  = dict(test_df.groupby('userId').apply(lambda d: list(d['movieId'])))\n",
    "\n",
    "dataset = {'train': train,\n",
    "            'val': val,\n",
    "            'test': test,\n",
    "            'umap': umap,\n",
    "            'smap': smap}\n",
    "\n",
    "with open('../data/bert_4_rec_data/dataset.pkl', \"wb\") as e:\n",
    "    pickle.dump(dataset, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement a Negative Sampling Class\n",
    "from tqdm import trange\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class PopularNegativeSampler(metaclass=ABCMeta):\n",
    "\n",
    "    def __init__(self, train, val, test, user_count, item_count, sample_size, seed, save_folder):\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        self.user_count = user_count\n",
    "        self.item_count = item_count\n",
    "        self.sample_size = sample_size\n",
    "        self.seed = seed\n",
    "        self.save_folder = save_folder\n",
    "    \n",
    "    def get_negative_samples(self):\n",
    "        savefile_path = self._get_save_path()\n",
    "        if savefile_path.is_file():\n",
    "            print('Negatives samples exist. Loading.')\n",
    "            negative_samples = pickle.load(savefile_path.open('rb'))\n",
    "            return negative_samples\n",
    "        print(\"Negative samples don't exist. Generating.\")\n",
    "        negative_samples = self.generate_negative_samples()\n",
    "        with savefile_path.open('wb') as f:\n",
    "            pickle.dump(negative_samples, f)\n",
    "        return negative_samples\n",
    "\n",
    "    def _get_save_path(self):\n",
    "        folder = Path(self.save_folder)\n",
    "        filename = 'Popular-sample_size{}-seed{}.pkl'.format(self.sample_size, self.seed)\n",
    "        return folder.joinpath(filename)\n",
    "\n",
    "    def generate_negative_samples(self):\n",
    "        popular_items = self.items_by_popularity()\n",
    "\n",
    "        negative_samples = {}\n",
    "        print('Sampling negative items')\n",
    "        for user in trange(self.user_count):\n",
    "            try:\n",
    "                seen = set(self.train[user])\n",
    "                seen.update(self.val[user])\n",
    "                seen.update(self.test[user])\n",
    "\n",
    "                samples = []\n",
    "                for item in popular_items:\n",
    "                    if len(samples) == self.sample_size:\n",
    "                        break\n",
    "                    if item in seen:\n",
    "                        continue\n",
    "                    samples.append(item)\n",
    "\n",
    "                negative_samples[user] = samples\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return negative_samples\n",
    "\n",
    "    def items_by_popularity(self):\n",
    "        popularity = Counter()\n",
    "\n",
    "        for user in self.train.keys():\n",
    "            popularity.update(self.train[user])\n",
    "\n",
    "        for user in self.val.keys():\n",
    "            popularity.update(self.val[user])\n",
    "\n",
    "        for user in self.test.keys():\n",
    "            popularity.update(self.test[user])\n",
    "            \n",
    "        popular_items = sorted(popularity, key=popularity.get, reverse=True)\n",
    "        return popular_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement a Dataloader class that will load data into model\n",
    "from abc import *\n",
    "import random\n",
    "\n",
    "class model_Dataloader(metaclass=ABCMeta):\n",
    "    def __init__(self, args, dataset):\n",
    "        self.args = args\n",
    "        seed = args.random_seed\n",
    "        self.rng = random.Random(seed)\n",
    "        self.train = dataset['train']\n",
    "        self.val = dataset['val']\n",
    "        self.test = dataset['test']\n",
    "        self.umap = dataset['umap']\n",
    "        self.smap = dataset['smap']\n",
    "        self.user_count = len(self.umap)\n",
    "        self.item_count = len(self.smap)\n",
    "        self.max_len = args.bert_max_len\n",
    "        self.mask_prob = args.bert_mask_prob\n",
    "        self.CLOZE_MASK_TOKEN = self.item_count + 1\n",
    "        self.save_folder = args.save_folder\n",
    "\n",
    "        code = args.train_negative_sampler_code\n",
    "        train_negative_sampler = PopularNegativeSampler(self.train, self.val, self.test,\n",
    "                                                          self.user_count, self.item_count,\n",
    "                                                          args.train_negative_sample_size,\n",
    "                                                          args.train_negative_sampling_seed,\n",
    "                                                          self.save_folder)\n",
    "        code = args.test_negative_sampler_code\n",
    "        test_negative_sampler = PopularNegativeSampler(self.train, self.val, self.test,\n",
    "                                                         self.user_count, self.item_count,\n",
    "                                                         args.test_negative_sample_size,\n",
    "                                                         args.test_negative_sampling_seed,\n",
    "                                                         self.save_folder)\n",
    "\n",
    "        self.train_negative_samples = train_negative_sampler.get_negative_samples()\n",
    "        self.test_negative_samples = test_negative_sampler.get_negative_samples()\n",
    "\n",
    "    def code(cls):\n",
    "        return 'bert'\n",
    "\n",
    "    def get_pytorch_dataloaders(self):\n",
    "        train_loader = self._get_train_loader()\n",
    "        val_loader = self._get_val_loader()\n",
    "        test_loader = self._get_test_loader()\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def _get_train_loader(self):\n",
    "        dataset = self._get_train_dataset()\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=self.args.batch_size,\n",
    "                                           shuffle=True, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_train_dataset(self):\n",
    "        dataset = BertTrainDataset(self.train, self.max_len, self.mask_prob, self.CLOZE_MASK_TOKEN, self.item_count, self.rng)\n",
    "        return dataset\n",
    "\n",
    "    def _get_val_loader(self):\n",
    "        return self._get_eval_loader(mode='val')\n",
    "\n",
    "    def _get_test_loader(self):\n",
    "        return self._get_eval_loader(mode='test')\n",
    "\n",
    "    def _get_eval_loader(self, mode):\n",
    "        batch_size = self.args.batch_size\n",
    "        dataset = self._get_eval_dataset(mode)\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=batch_size,\n",
    "                                           shuffle=False, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_eval_dataset(self, mode):\n",
    "        answers = self.val if mode == 'val' else self.test\n",
    "        dataset = BertEvalDataset(self.train, answers, self.max_len, self.CLOZE_MASK_TOKEN, self.test_negative_samples)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class BertTrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_prob, mask_token, num_items, rng):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token = mask_token\n",
    "        self.num_items = num_items\n",
    "        self.rng = rng\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self._getseq(user)\n",
    "\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for s in seq:\n",
    "            prob = self.rng.random()\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "\n",
    "                if prob < 0.8:\n",
    "                    tokens.append(self.mask_token)\n",
    "                elif prob < 0.9:\n",
    "                    tokens.append(self.rng.randint(1, self.num_items))\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "\n",
    "                labels.append(s)\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                labels.append(0)\n",
    "\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        labels = labels[-self.max_len:]\n",
    "\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "\n",
    "        tokens = [0] * mask_len + tokens\n",
    "        labels = [0] * mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "    def _getseq(self, user):\n",
    "        return self.u2seq[user]\n",
    "\n",
    "\n",
    "class BertEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, u2answer, max_len, mask_token, negative_samples):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "        answer = self.u2answer[user]\n",
    "        negs = self.negative_samples[user]\n",
    "\n",
    "        candidates = answer + negs\n",
    "        labels = [1] * len(answer) + [0] * len(negs)\n",
    "\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [0] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Negatives samples exist. Loading.\nNegatives samples exist. Loading.\n"
     ]
    }
   ],
   "source": [
    "dataloader = model_Dataloader(args, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataloader.get_pytorch_dataloaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 716/716 [00:00<00:00, 13545.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Since we are considering the changing user preference overtime, we have to model the data as a timeseries and split train/test by specifying a specific timepoint to split the data (Hold-Out Sampling)\n",
    "\n",
    "np.random.seed(10)\n",
    "eval_set_size = int(len(result)*test_size)\n",
    "user_count = len(result.userId.unique())\n",
    "\n",
    "# Generate user indices\n",
    "permuted_index = np.random.permutation(user_count)\n",
    "train_user_index = permuted_index[                :-2*eval_set_size]\n",
    "val_user_index   = permuted_index[-2*eval_set_size:  -eval_set_size]\n",
    "test_user_index  = permuted_index[  -eval_set_size:                ]\n",
    "\n",
    "# Split DataFrames\n",
    "train_df = result.loc[result['userId'].isin(train_user_index)]\n",
    "val_df   = result.loc[result['userId'].isin(val_user_index)]\n",
    "test_df  = result.loc[result['userId'].isin(test_user_index)]\n",
    "\n",
    "# DataFrame to dict => {uid : list of sid's}\n",
    "train = dict(train_df.groupby('userId').progress_apply(lambda d: list(d['movieId'])))\n",
    "val   = dict(val_df.groupby('userId').progress_apply(lambda d: list(d['movieId'])))\n",
    "test  = dict(test_df.groupby('userId').progress_apply(lambda d: list(d['movieId'])))\n",
    "\n",
    "with open('../data/bert_4_rec_data/train.pkl', \"wb\") as e:\n",
    "    pickle.dump(train, e)\n",
    "\n",
    "with open('../data/bert_4_rec_data/validation.pkl', \"wb\") as e:\n",
    "    pickle.dump(val, e)\n",
    "\n",
    "with open('../data/bert_4_rec_data/test.pkl', \"wb\") as e:\n",
    "    pickle.dump(test, e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Data\n",
    "#Probability Mass Distibution of user and movie\n",
    "def densify_index(df):\n",
    "    print('Densifying index')\n",
    "    umap = {u: i for i, u in enumerate(set(df['userId']))}\n",
    "    smap = {s: i for i, s in enumerate(set(df['movieId']))}\n",
    "    df['uid'] = df['uid'].map(umap)\n",
    "    df['sid'] = df['sid'].map(smap)\n",
    "    return df, umap, smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10644"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "umap = {u: i for i, u in enumerate(set(result['userId']))}\n",
    "len(umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "886426"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement a Dataloader class\n",
    "\n",
    "class model_Dataloader(metaclass=ABCMeta):\n",
    "    def __init__(self, args, dataset):\n",
    "        self.args = args\n",
    "        seed = args.random_seed\n",
    "        self.rng = random.Random(seed)\n",
    "        self.train = dataset['train']\n",
    "        self.val = dataset['val']\n",
    "        self.test = dataset['test']\n",
    "        self.umap = dataset['umap']\n",
    "        self.smap = dataset['smap']\n",
    "        self.user_count = len(self.umap)\n",
    "        self.item_count = len(self.smap)\n",
    "        self.max_len = args.bert_max_len\n",
    "        self.mask_prob = args.bert_mask_prob\n",
    "        self.CLOZE_MASK_TOKEN = self.item_count + 1\n",
    "\n",
    "        code = args.train_negative_sampler_code\n",
    "        train_negative_sampler = negative_sampler_factory(code, self.train, self.val, self.test,\n",
    "                                                          self.user_count, self.item_count,\n",
    "                                                          args.train_negative_sample_size,\n",
    "                                                          args.train_negative_sampling_seed,\n",
    "                                                          self.save_folder)\n",
    "        code = args.test_negative_sampler_code\n",
    "        test_negative_sampler = negative_sampler_factory(code, self.train, self.val, self.test,\n",
    "                                                         self.user_count, self.item_count,\n",
    "                                                         args.test_negative_sample_size,\n",
    "                                                         args.test_negative_sampling_seed,\n",
    "                                                         self.save_folder)\n",
    "\n",
    "        self.train_negative_samples = train_negative_sampler.get_negative_samples()\n",
    "        self.test_negative_samples = test_negative_sampler.get_negative_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .base import AbstractDataloader\n",
    "from .negative_samplers import negative_sampler_factory\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "class BertDataloader(AbstractDataloader):\n",
    "    def __init__(self, args, dataset):\n",
    "        super().__init__(args, dataset)\n",
    "        args.num_items = len(self.smap)\n",
    "        self.max_len = args.bert_max_len\n",
    "        self.mask_prob = args.bert_mask_prob\n",
    "        self.CLOZE_MASK_TOKEN = self.item_count + 1\n",
    "\n",
    "        code = args.train_negative_sampler_code\n",
    "        train_negative_sampler = negative_sampler_factory(code, self.train, self.val, self.test,\n",
    "                                                          self.user_count, self.item_count,\n",
    "                                                          args.train_negative_sample_size,\n",
    "                                                          args.train_negative_sampling_seed,\n",
    "                                                          self.save_folder)\n",
    "        code = args.test_negative_sampler_code\n",
    "        test_negative_sampler = negative_sampler_factory(code, self.train, self.val, self.test,\n",
    "                                                         self.user_count, self.item_count,\n",
    "                                                         args.test_negative_sample_size,\n",
    "                                                         args.test_negative_sampling_seed,\n",
    "                                                         self.save_folder)\n",
    "\n",
    "        self.train_negative_samples = train_negative_sampler.get_negative_samples()\n",
    "        self.test_negative_samples = test_negative_sampler.get_negative_samples()\n",
    "\n",
    "    @classmethod\n",
    "    def code(cls):\n",
    "        return 'bert'\n",
    "\n",
    "    def get_pytorch_dataloaders(self):\n",
    "        train_loader = self._get_train_loader()\n",
    "        val_loader = self._get_val_loader()\n",
    "        test_loader = self._get_test_loader()\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def _get_train_loader(self):\n",
    "        dataset = self._get_train_dataset()\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=self.args.batch_size,\n",
    "                                           shuffle=True, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_train_dataset(self):\n",
    "        dataset = BertTrainDataset(self.train, self.max_len, self.mask_prob, self.CLOZE_MASK_TOKEN, self.item_count, self.rng)\n",
    "        return dataset\n",
    "\n",
    "    def _get_val_loader(self):\n",
    "        return self._get_eval_loader(mode='val')\n",
    "\n",
    "    def _get_test_loader(self):\n",
    "        return self._get_eval_loader(mode='test')\n",
    "\n",
    "    def _get_eval_loader(self, mode):\n",
    "        batch_size = self.args.val_batch_size if mode == 'val' else self.args.test_batch_size\n",
    "        dataset = self._get_eval_dataset(mode)\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=batch_size,\n",
    "                                           shuffle=False, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_eval_dataset(self, mode):\n",
    "        answers = self.val if mode == 'val' else self.test\n",
    "        dataset = BertEvalDataset(self.train, answers, self.max_len, self.CLOZE_MASK_TOKEN, self.test_negative_samples)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class BertTrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_prob, mask_token, num_items, rng):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token = mask_token\n",
    "        self.num_items = num_items\n",
    "        self.rng = rng\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self._getseq(user)\n",
    "\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for s in seq:\n",
    "            prob = self.rng.random()\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "\n",
    "                if prob < 0.8:\n",
    "                    tokens.append(self.mask_token)\n",
    "                elif prob < 0.9:\n",
    "                    tokens.append(self.rng.randint(1, self.num_items))\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "\n",
    "                labels.append(s)\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                labels.append(0)\n",
    "\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        labels = labels[-self.max_len:]\n",
    "\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "\n",
    "        tokens = [0] * mask_len + tokens\n",
    "        labels = [0] * mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "    def _getseq(self, user):\n",
    "        return self.u2seq[user]\n",
    "\n",
    "\n",
    "\n",
    "class BertEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, u2answer, max_len, mask_token, negative_samples):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "        answer = self.u2answer[user]\n",
    "        negs = self.negative_samples[user]\n",
    "\n",
    "        candidates = answer + negs\n",
    "        labels = [1] * len(answer) + [0] * len(negs)\n",
    "\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [0] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processing the data\n",
    "\n",
    "def convert_time(x):\n",
    "    timestamp = int(time.mktime(x.timetuple()))\n",
    "    return timestamp\n",
    "\n",
    "def clean_user(x):\n",
    "    x = x.strip(\"::\")\n",
    "    return x \n",
    "\n",
    "def strain(df, column, cutoffs):\n",
    "    criteria = (lambda x: (len(x) > cutoffs[0]) & (len(x) < cutoffs[1]))\n",
    "    subset = df.groupby(column).filter(criteria)\n",
    "    return subset\n",
    "\n",
    "def convert(path, data):\n",
    "    with open(path, 'w') as filehandle:\n",
    "        for x in data:\n",
    "            user = str(x[0])\n",
    "            item = str(x[1])\n",
    "            rating = str(x[2])\n",
    "            timestamp = str(x[3])\n",
    "            line = user + \"::\" + item + \"::\" + rating + \"::\" + timestamp\n",
    "            filehandle.write('%s\\n' % line)\n",
    "\n",
    "df = pd.read_json('movies_total.json')\n",
    "df['Date'] = df['Date'].apply(convert_time)\n",
    "df['User'] = df['User'].apply(clean_user)\n",
    "df.rename(columns = {'Date': 'Timestamp'}, inplace=True)\n",
    "data = sub[['User', 'Movie ID', 'Rating', 'Timestamp']].values.tolist()\n",
    "sub = strain(df, \"Movie ID\", [10, 10000])\n",
    "sub = strain(sub, \"User\", [5, 10000])\n",
    "\n",
    "print(\"The total number of users:\", len(sub['User'].value_counts()))\n",
    "print(\"The total number of movies:\", len(sub['Movie ID'].value_counts()))\n",
    "\n",
    "path = 'full_ratings.dat'\n",
    "convert(path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For this model we only need the history of ratings\n",
    "def getRatings(df):\n",
    "    result = pd.DataFrame()\n",
    "    result[userId] = df[]\n",
    "\n",
    "\n",
    "## Get Size of the Data\n",
    "def get_count(tp, id):\n",
    "    groups = tp[[id]].groupby(id, as_index=False)\n",
    "    count = groups.size()\n",
    "    return count\n",
    "\n",
    "#Get movies and users which share at least 3 intersection\n",
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
    "    return tp, usercount, itemcount\n",
    "\n",
    "#Possible use to reduce size of data (we are only interested in the movies that were rated 4-5 )\n",
    "def make_implicit(df, min_rating):\n",
    "    print('Turning into implicit ratings')\n",
    "    df = df[df['rating'] >= min_rating]\n",
    "    # return df[['uid', 'sid', 'timestamp']]\n",
    "    return df\n",
    "\n",
    "\n",
    "#Probability Mass Distibution of user and movie\n",
    "def densify_index(df):\n",
    "    print('Densifying index')\n",
    "    umap = {u: i for i, u in enumerate(set(df['uid']))}\n",
    "    smap = {s: i for i, s in enumerate(set(df['sid']))}\n",
    "    df['uid'] = df['uid'].map(umap)\n",
    "    df['sid'] = df['sid'].map(smap)\n",
    "    return df, umap, smap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "#Create a Dataset Class that cr\n",
    "class ML20MDataset(metaclass=ABCMeta):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.min_rating = args.min_rating\n",
    "        self.min_uc = args.min_uc\n",
    "        self.min_sc = args.min_sc\n",
    "        self.split = args.split\n",
    "\n",
    "        assert self.min_uc >= 2, 'Need at least 2 ratings per user for validation and test'\n",
    "    \n",
    "    def make_implicit(self, df):\n",
    "        print('Turning into implicit ratings')\n",
    "        df = df[df['rating'] >= self.min_rating]\n",
    "        # return df[['uid', 'sid', 'timestamp']]\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    def densify_index(self, df):\n",
    "        print('Densifying index')\n",
    "        umap = {u: i for i, u in enumerate(set(df['uid']))}\n",
    "        smap = {s: i for i, s in enumerate(set(df['sid']))}\n",
    "        df['uid'] = df['uid'].map(umap)\n",
    "        df['sid'] = df['sid'].map(smap)\n",
    "        return df, umap, smap\n",
    "\n",
    "    def split_df(self, df, user_count):\n",
    "        if self.args.split == 'leave_one_out':\n",
    "            print('Splitting')\n",
    "            user_group = df.groupby('uid')\n",
    "            user2items = user_group.progress_apply(lambda d: list(d.sort_values(by='timestamp')['sid']))\n",
    "            train, val, test = {}, {}, {}\n",
    "            for user in range(user_count):\n",
    "                items = user2items[user]\n",
    "                train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n",
    "            return train, val, test\n",
    "        elif self.args.split == 'holdout':\n",
    "            print('Splitting')\n",
    "            np.random.seed(self.args.dataset_split_seed)\n",
    "            eval_set_size = self.args.eval_set_size\n",
    "\n",
    "            # Generate user indices\n",
    "            permuted_index = np.random.permutation(user_count)\n",
    "            train_user_index = permuted_index[                :-2*eval_set_size]\n",
    "            val_user_index   = permuted_index[-2*eval_set_size:  -eval_set_size]\n",
    "            test_user_index  = permuted_index[  -eval_set_size:                ]\n",
    "\n",
    "            # Split DataFrames\n",
    "            train_df = df.loc[df['uid'].isin(train_user_index)]\n",
    "            val_df   = df.loc[df['uid'].isin(val_user_index)]\n",
    "            test_df  = df.loc[df['uid'].isin(test_user_index)]\n",
    "\n",
    "            # DataFrame to dict => {uid : list of sid's}\n",
    "            train = dict(train_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            val   = dict(val_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            test  = dict(test_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            return train, val, test\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def all_raw_file_names(cls):\n",
    "        return ['genome-scores.csv',\n",
    "                'genome-tags.csv',\n",
    "                'links.csv',\n",
    "                'movies.csv',\n",
    "                'ratings.csv',\n",
    "                'README.txt',\n",
    "                'tags.csv']\n",
    "\n",
    "    \n",
    "    def load_ratings_df(self):\n",
    "        folder_path = self._get_rawdata_folder_path()\n",
    "        file_path = folder_path.joinpath('ratings.csv')\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
    "        return df\n",
    "\n",
    "       def load_dataset(self):\n",
    "        self.preprocess()\n",
    "        dataset_path = self._get_preprocessed_dataset_path()\n",
    "        dataset = pickle.load(dataset_path.open('rb'))\n",
    "        return dataset\n",
    "\n",
    "    def preprocess(self):\n",
    "        dataset_path = self._get_preprocessed_dataset_path()\n",
    "        if dataset_path.is_file():\n",
    "            print('Already preprocessed. Skip preprocessing')\n",
    "            return\n",
    "        if not dataset_path.parent.is_dir():\n",
    "            dataset_path.parent.mkdir(parents=True)\n",
    "        self.maybe_download_raw_dataset()\n",
    "        df = self.load_ratings_df()\n",
    "        df = self.make_implicit(df)\n",
    "        df = self.filter_triplets(df)\n",
    "        df, umap, smap = self.densify_index(df)\n",
    "        train, val, test = self.split_df(df, len(umap))\n",
    "        dataset = {'train': train,\n",
    "                   'val': val,\n",
    "                   'test': test,\n",
    "                   'umap': umap,\n",
    "                   'smap': smap}\n",
    "        with dataset_path.open('wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "\n",
    "    def _get_rawdata_root_path(self):\n",
    "        return Path(RAW_DATASET_ROOT_FOLDER)\n",
    "\n",
    "    def _get_rawdata_folder_path(self):\n",
    "        root = self._get_rawdata_root_path()\n",
    "        return root.joinpath(self.raw_code())\n",
    "\n",
    "    def _get_preprocessed_root_path(self):\n",
    "        root = self._get_rawdata_root_path()\n",
    "        return root.joinpath('preprocessed')\n",
    "\n",
    "    def _get_preprocessed_folder_path(self):\n",
    "        preprocessed_root = self._get_preprocessed_root_path()\n",
    "        folder_name = '{}_min_rating{}-min_uc{}-min_sc{}-split{}' \\\n",
    "            .format(self.code(), self.min_rating, self.min_uc, self.min_sc, self.split)\n",
    "        return preprocessed_root.joinpath(folder_name)\n",
    "\n",
    "    def _get_preprocessed_dataset_path(self):\n",
    "        folder = self._get_preprocessed_folder_path()\n",
    "        return folder.joinpath('dataset.pkl')"
   ]
  }
 ]
}