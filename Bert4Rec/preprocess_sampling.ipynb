{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".venv",
   "display_name": ".venv",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from abc import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "import pprint as pp\n",
    "\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime, date\n",
    "\n",
    "from abc import *\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "### Model Building\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Class that will hold our parameter values as attributes, thus making it easy to call\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.random_seed = 10\n",
    "        \n",
    "        #Data Split\n",
    "        self.train_size = 0.8\n",
    "        self.test_size = 1- self.train_size\n",
    "        self.save_folder = '../data/bert_4_rec_data'\n",
    "\n",
    "        #Sampling\n",
    "        self.train_negative_sampler_code = 'random'\n",
    "        self.train_negative_sample_size = 0\n",
    "        self.train_negative_sampling_seed = 0\n",
    "        self.test_negative_sampler_code = 'random'\n",
    "        self.test_negative_sample_size = 100\n",
    "        self.test_negative_sampling_seed = 98765\n",
    "\n",
    "        #BERT Model\n",
    "        self.bert_max_len = 100\n",
    "        self.bert_mask_prob = 0.15\n",
    "        self.batch_size = 128\n",
    "\n",
    "        self.bert_dropout = 0.1\n",
    "        self.bert_hidden_units = 256\n",
    "        self.bert_mask_prob = 0.15\n",
    "        self.bert_max_len = 100\n",
    "        self.bert_num_blocks = 2\n",
    "        self.bert_num_heads = 4\n",
    "\n",
    "        # Untunable Parameters:\n",
    "        self.num_items =None\n",
    "\n",
    "        # Parameters for trainer:\n",
    "        self.num_gpu = 1\n",
    "        self.device = 'cuda'\n",
    "        self.device_idx = '0'\n",
    "        self.optimizer = 'Adam'\n",
    "        self.lr = 0.001\n",
    "        self.enable_lr_schedule = True\n",
    "        self.decay_step = 25\n",
    "        self.gamma = 1.0\n",
    "        self.num_epochs = 100\n",
    "        self.metric_ks = [1, 5, 10, 20, 50, 100]\n",
    "        self.best_metric = 'NDCG@10'\n",
    "        self.log_period_as_iter = 10000 # Define how often to log data \n",
    "        \n",
    "        #Parameters for Optimizer:\n",
    "        self.weight_decay = 0\n",
    "\n",
    "        # Saving Model:\n",
    "        self.experiment_dir = \"model\"\n",
    "        self.experiment_description = \"BT4222\"\n",
    "\n",
    "args = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset = pd.read_pickle('../data/bert_4_rec_data/dataset.pkl')\n",
    "\n",
    "except:\n",
    "    data = pd.read_csv('../data/processed_data/full_data.csv')\n",
    "    data = data.drop(columns = \"Unnamed: 0\")\n",
    "\n",
    "    ## Retriving the relevant features\n",
    "    ratings = pd.DataFrame()\n",
    "\n",
    "    ratings['userId'] = data['userId']\n",
    "    ratings['movieId'] = data['movieId']\n",
    "    ratings['rating'] = data['rating']\n",
    "    ratings['timestamp'] = data['timestamp']\n",
    "\n",
    "    # Since we are considering good movies we will filter out those rating below 3.5 (Determined by the histogram of ratings)\n",
    "    ratings = ratings[ratings['rating'] >= 3.5]\n",
    "\n",
    "    # Probability Density  Distribution of user and movie\n",
    "\n",
    "    print('Densifying index')\n",
    "    umap = {u: i for i, u in enumerate(set(ratings['userId']))}\n",
    "    smap = {s: i for i, s in enumerate(set(ratings['movieId']))}\n",
    "    ratings['userId'] = ratings['userId'].map(umap)\n",
    "    ratings['movieId'] = ratings['movieId'].map(smap)\n",
    "\n",
    "    # Since we are considering the changing user preference overtime, we have to model the data as a timeseries and split train/test by specifying a specific timepoint to split the data (leave-one-Out Sampling)\n",
    "    user_count = len(umap)\n",
    "    print('Splitting')\n",
    "    user_group = ratings.groupby('userId')\n",
    "    user2items = user_group.progress_apply(lambda d: list(d.sort_values(by='timestamp')['movieId']))\n",
    "    train, val, test = {}, {}, {}\n",
    "    for user in range(user_count):\n",
    "        items = user2items[user]\n",
    "        train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n",
    "\n",
    "    dataset = {'train': train,\n",
    "            'val': val,\n",
    "            'test': test,\n",
    "            'umap': umap,\n",
    "            'smap': smap}\n",
    "\n",
    "    with open('../data/bert_4_rec_data/dataset.pkl', \"wb\") as e:\n",
    "        pickle.dump(dataset, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement a Negative Sampling Class\n",
    "from tqdm import trange\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class PopularNegativeSampler(metaclass=ABCMeta):\n",
    "\n",
    "    def __init__(self, train, val, test, user_count, item_count, sample_size, seed, save_folder):\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        self.user_count = user_count\n",
    "        self.item_count = item_count\n",
    "        self.sample_size = sample_size\n",
    "        self.seed = seed\n",
    "        self.save_folder = save_folder\n",
    "    \n",
    "    def get_negative_samples(self):\n",
    "        savefile_path = self._get_save_path()\n",
    "        if savefile_path.is_file():\n",
    "            print('Negatives samples exist. Loading.')\n",
    "            negative_samples = pickle.load(savefile_path.open('rb'))\n",
    "            return negative_samples\n",
    "        print(\"Negative samples don't exist. Generating.\")\n",
    "        negative_samples = self.generate_negative_samples()\n",
    "        with savefile_path.open('wb') as f:\n",
    "            pickle.dump(negative_samples, f)\n",
    "        return negative_samples\n",
    "\n",
    "    def _get_save_path(self):\n",
    "        folder = Path(self.save_folder)\n",
    "        filename = 'Popular-sample_size{}-seed{}.pkl'.format(self.sample_size, self.seed)\n",
    "        return folder.joinpath(filename)\n",
    "\n",
    "    def generate_negative_samples(self):\n",
    "        popular_items = self.items_by_popularity()\n",
    "\n",
    "        negative_samples = {}\n",
    "        print('Sampling negative items')\n",
    "        for user in trange(self.user_count):\n",
    "            try:\n",
    "                seen = set(self.train[user])\n",
    "                seen.update(self.val[user])\n",
    "                seen.update(self.test[user])\n",
    "\n",
    "                samples = []\n",
    "                for item in popular_items:\n",
    "                    if len(samples) == self.sample_size:\n",
    "                        break\n",
    "                    if item in seen:\n",
    "                        continue\n",
    "                    samples.append(item)\n",
    "\n",
    "                negative_samples[user] = samples\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return negative_samples\n",
    "\n",
    "    def items_by_popularity(self):\n",
    "        popularity = Counter()\n",
    "\n",
    "        for user in self.train.keys():\n",
    "            popularity.update(self.train[user])\n",
    "\n",
    "        for user in self.val.keys():\n",
    "            popularity.update(self.val[user])\n",
    "\n",
    "        for user in self.test.keys():\n",
    "            popularity.update(self.test[user])\n",
    "            \n",
    "        popular_items = sorted(popularity, key=popularity.get, reverse=True)\n",
    "        return popular_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement a Dataloader class that will load data into model\n",
    "from abc import *\n",
    "import random\n",
    "\n",
    "class model_Dataloader(metaclass=ABCMeta):\n",
    "    def __init__(self, args, dataset):\n",
    "        self.args = args\n",
    "        seed = args.random_seed\n",
    "        self.rng = random.Random(seed)\n",
    "        self.train = dataset['train']\n",
    "        self.val = dataset['val']\n",
    "        self.test = dataset['test']\n",
    "        self.umap = dataset['umap']\n",
    "        self.smap = dataset['smap']\n",
    "        self.user_count = len(self.umap)\n",
    "        self.item_count = len(self.smap)\n",
    "        self.max_len = args.bert_max_len\n",
    "        self.mask_prob = args.bert_mask_prob\n",
    "        self.CLOZE_MASK_TOKEN = self.item_count + 1\n",
    "        self.save_folder = args.save_folder\n",
    "        args.num_items = len(self.smap)\n",
    "\n",
    "        code = args.train_negative_sampler_code\n",
    "        train_negative_sampler = PopularNegativeSampler(self.train, self.val, self.test,\n",
    "                                                          self.user_count, self.item_count,\n",
    "                                                          args.train_negative_sample_size,\n",
    "                                                          args.train_negative_sampling_seed,\n",
    "                                                          self.save_folder)\n",
    "        code = args.test_negative_sampler_code\n",
    "        test_negative_sampler = PopularNegativeSampler(self.train, self.val, self.test,\n",
    "                                                         self.user_count, self.item_count,\n",
    "                                                         args.test_negative_sample_size,\n",
    "                                                         args.test_negative_sampling_seed,\n",
    "                                                         self.save_folder)\n",
    "\n",
    "        self.train_negative_samples = train_negative_sampler.get_negative_samples()\n",
    "        self.test_negative_samples = test_negative_sampler.get_negative_samples()\n",
    "\n",
    "    def code(cls):\n",
    "        return 'bert'\n",
    "\n",
    "    def get_pytorch_dataloaders(self):\n",
    "        train_loader = self._get_train_loader()\n",
    "        val_loader = self._get_val_loader()\n",
    "        test_loader = self._get_test_loader()\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def _get_train_loader(self):\n",
    "        dataset = self._get_train_dataset()\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=self.args.batch_size,\n",
    "                                           shuffle=True, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_train_dataset(self):\n",
    "        dataset = BertTrainDataset(self.train, self.max_len, self.mask_prob, self.CLOZE_MASK_TOKEN, self.item_count, self.rng)\n",
    "        return dataset\n",
    "\n",
    "    def _get_val_loader(self):\n",
    "        return self._get_eval_loader(mode='val')\n",
    "\n",
    "    def _get_test_loader(self):\n",
    "        return self._get_eval_loader(mode='test')\n",
    "\n",
    "    def _get_eval_loader(self, mode):\n",
    "        batch_size = self.args.batch_size\n",
    "        dataset = self._get_eval_dataset(mode)\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=batch_size,\n",
    "                                           shuffle=False, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_eval_dataset(self, mode):\n",
    "        answers = self.val if mode == 'val' else self.test\n",
    "        dataset = BertEvalDataset(self.train, answers, self.max_len, self.CLOZE_MASK_TOKEN, self.test_negative_samples)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class BertTrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_prob, mask_token, num_items, rng):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token = mask_token\n",
    "        self.num_items = num_items\n",
    "        self.rng = rng\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self._getseq(user)\n",
    "\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for s in seq:\n",
    "            prob = self.rng.random()\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "\n",
    "                if prob < 0.8:\n",
    "                    tokens.append(self.mask_token)\n",
    "                elif prob < 0.9:\n",
    "                    tokens.append(self.rng.randint(1, self.num_items))\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "\n",
    "                labels.append(s)\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                labels.append(0)\n",
    "\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        labels = labels[-self.max_len:]\n",
    "\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "\n",
    "        tokens = [0] * mask_len + tokens\n",
    "        labels = [0] * mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "    def _getseq(self, user):\n",
    "        return self.u2seq[user]\n",
    "\n",
    "\n",
    "class BertEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, u2answer, max_len, mask_token, negative_samples):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "        answer = self.u2answer[user]\n",
    "        negs = self.negative_samples[user]\n",
    "\n",
    "        candidates = answer + negs\n",
    "        labels = [1] * len(answer) + [0] * len(negs)\n",
    "\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [0] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Negatives samples exist. Loading.\nNegatives samples exist. Loading.\n"
     ]
    }
   ],
   "source": [
    "dataloader = model_Dataloader(args, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataloader.get_pytorch_dataloaders()\n"
   ]
  },
  {
   "source": [
    "## BERT Model Implementation as adapted from: https://github.com/codertimo/BERT-pytorch/tree/master/bert_pytorch/model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Embedding Layer BERT consists of 3 Embedding Layers <br>\n",
    "1. Token Embedding : normal embedding matrix <br>\n",
    "2. Positional Embedding : adding positional information <br>\n",
    "3. Segment Embedding : adding sentence segment info, (sent_A:1, sent_B:2)<br>\n",
    "\n",
    "Embedding size is 512 as BERT was designed to process input sequences of up to length 512 <br>\n",
    "\n",
    "Implemented using pytorch embedding class: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "More on Embedding can be found : https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Token Embedding -> Word Piece Tokenization -> torch normal Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "source": [
    "Positional Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)"
   ]
  },
  {
   "source": [
    "Segment Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentEmbedding(nn.Embedding):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__(3, embed_size, padding_idx=0)"
   ]
  },
  {
   "source": [
    "BERT Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #Token Embedding\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        # self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        x = self.token(sequence) + self.position(sequence)  # + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "source": [
    "### Transformer\n",
    "\n",
    "The activation function used in BERT is GELU: <br>\n",
    "More about it can be found : https://paperswithcode.com/method/gelu <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "source": [
    "Based on the paper : https://arxiv.org/abs/1607.06450 <br>\n",
    "\n",
    "Layer Normalization is often used in transformer achitecture. <br>\n",
    "\n",
    "The idea of normalization is to improve the model's ability to perform on unseen data (Generalization) <br>\n",
    "as well as reduce the training time of the model\n",
    "\n",
    "The rationale for implementing Layer Normalization over Batch Normalization is that the <br>\n",
    "effectiveness of batch normalization is dependent on batch size.\n",
    "\n",
    "Layer Normalization is implemented with torch Layer Norm Class : https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "source": [
    "### Attention Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        random.seed(args.random_seed)\n",
    "        torch.manual_seed(args.random_seed)\n",
    "        torch.cuda.manual_seed_all(args.random_seed)\n",
    "        np.random.seed(args.random_seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "\n",
    "        max_len = args.bert_max_len\n",
    "        num_items = args.num_items\n",
    "        n_layers = args.bert_num_blocks\n",
    "        heads = args.bert_num_heads\n",
    "        vocab_size = num_items + 2\n",
    "        hidden = args.bert_hidden_units\n",
    "        self.hidden = hidden\n",
    "        dropout = args.bert_dropout\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=self.hidden, max_len=max_len, dropout=dropout)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, heads, hidden * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass"
   ]
  },
  {
   "source": [
    "### Putting it all together --BERT Model--"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module, metaclass=ABCMeta):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.bert = BERT(args)\n",
    "        self.out = nn.Linear(self.bert.hidden, args.num_items + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTModel(args)"
   ]
  },
  {
   "source": [
    "## Build a trainer class that will facilitate the training of the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_DICT_KEY = 'model_state_dict'\n",
    "OPTIMIZER_STATE_DICT_KEY = 'optimizer_state_dict'"
   ]
  },
  {
   "source": [
    "## Define the Scoring metric that will be used\n",
    "\n",
    "Recall :\n",
    "\n",
    "Nomarlized Discounted Cumulative Gain :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "#### Recall"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(scores, labels, k):\n",
    "    scores = scores\n",
    "    labels = labels\n",
    "    rank = (-scores).argsort(dim=1)\n",
    "    cut = rank[:, :k]\n",
    "    hit = labels.gather(1, cut)\n",
    "    return (hit.sum(1).float() / torch.min(torch.Tensor([k]).to(hit.device), labels.sum(1).float())).mean().cpu().item()"
   ]
  },
  {
   "source": [
    "#### Nomarlized Discounted Cumulative Gain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(scores, labels, k):\n",
    "    scores = scores.cpu()\n",
    "    labels = labels.cpu()\n",
    "    rank = (-scores).argsort(dim=1)\n",
    "    cut = rank[:, :k]\n",
    "    hits = labels.gather(1, cut)\n",
    "    position = torch.arange(2, 2+k)\n",
    "    weights = 1 / torch.log2(position.float())\n",
    "    dcg = (hits.float() * weights).sum(1)\n",
    "    idcg = torch.Tensor([weights[:min(int(n), k)].sum() for n in labels.sum(1)])\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg.mean()"
   ]
  },
  {
   "source": [
    "### Scoring Metric in terms of predicting the top k movies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalls_and_ndcgs_for_ks(scores, labels, ks):\n",
    "    metrics = {}\n",
    "\n",
    "    scores = scores\n",
    "    labels = labels\n",
    "    answer_count = labels.sum(1)\n",
    "\n",
    "    labels_float = labels.float()\n",
    "    rank = (-scores).argsort(dim=1)\n",
    "    cut = rank\n",
    "    for k in sorted(ks, reverse=True):\n",
    "       cut = cut[:, :k]\n",
    "       hits = labels_float.gather(1, cut)\n",
    "       metrics['Recall@%d' % k] = \\\n",
    "           (hits.sum(1) / torch.min(torch.Tensor([k]).to(labels.device), labels.sum(1).float())).mean().cpu().item()\n",
    "\n",
    "       position = torch.arange(2, 2+k)\n",
    "       weights = 1 / torch.log2(position.float())\n",
    "       dcg = (hits * weights.to(hits.device)).sum(1)\n",
    "       idcg = torch.Tensor([weights[:min(int(n), k)].sum() for n in answer_count]).to(dcg.device)\n",
    "       ndcg = (dcg / idcg).mean()\n",
    "       metrics['NDCG@%d' % k] = ndcg.cpu().item()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "source": [
    "## Define a AverageMeter class to track the performance of the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Define a AverageMeter class to track the performance of the model\n",
    "\n",
    "class AverageMeterSet(object):\n",
    "    def __init__(self, meters=None):\n",
    "        self.meters = meters if meters else {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key not in self.meters:\n",
    "            meter = AverageMeter()\n",
    "            meter.update(0)\n",
    "            return meter\n",
    "        return self.meters[key]\n",
    "\n",
    "    def update(self, name, value, n=1):\n",
    "        if name not in self.meters:\n",
    "            self.meters[name] = AverageMeter()\n",
    "        self.meters[name].update(value, n)\n",
    "\n",
    "    def reset(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "    def values(self, format_string='{}'):\n",
    "        return {format_string.format(name): meter.val for name, meter in self.meters.items()}\n",
    "\n",
    "    def averages(self, format_string='{}'):\n",
    "        return {format_string.format(name): meter.avg for name, meter in self.meters.items()}\n",
    "\n",
    "    def sums(self, format_string='{}'):\n",
    "        return {format_string.format(name): meter.sum for name, meter in self.meters.items()}\n",
    "\n",
    "    def counts(self, format_string='{}'):\n",
    "        return {format_string.format(name): meter.count for name, meter in self.meters.items()}\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __format__(self, format):\n",
    "        return \"{self.val:{format}} ({self.avg:{format}})\".format(self=self, format=format)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 27,
   "outputs": []
  },
  {
   "source": [
    "## Define Logger Class that stores best states of the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state_dict(state_dict, path, filename):\n",
    "    torch.save(state_dict, os.path.join(path, filename))"
   ]
  },
  {
   "source": [
    "### Administrative codes to facilitate the saving of model and results to file directories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_export_folder(args):\n",
    "    experiment_dir, experiment_description = args.experiment_dir, args.experiment_description\n",
    "    if not os.path.exists(experiment_dir):\n",
    "        os.mkdir(experiment_dir)\n",
    "    experiment_path = get_name_of_experiment_path(experiment_dir, experiment_description)\n",
    "    os.mkdir(experiment_path)\n",
    "    print('Folder created: ' + os.path.abspath(experiment_path))\n",
    "    return experiment_path\n",
    "\n",
    "\n",
    "def get_name_of_experiment_path(experiment_dir, experiment_description):\n",
    "    experiment_path = os.path.join(experiment_dir, (experiment_description + \"_\" + str(date.today())))\n",
    "    idx = _get_experiment_index(experiment_path)\n",
    "    experiment_path = experiment_path + \"_\" + str(idx)\n",
    "    return experiment_path\n",
    "\n",
    "\n",
    "def _get_experiment_index(experiment_path):\n",
    "    idx = 0\n",
    "    while os.path.exists(experiment_path + \"_\" + str(idx)):\n",
    "        idx += 1\n",
    "    return idx\n",
    "\n",
    "def save_test_result(export_root, result):\n",
    "    filepath = Path(export_root).joinpath('test_result.txt')\n",
    "    with filepath.open('w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "\n",
    "def export_experiments_config_as_json(args, experiment_path):\n",
    "    with open(os.path.join(experiment_path, 'config.json'), 'w') as outfile:\n",
    "        json.dump(vars(args), outfile, indent=2)\n",
    "\n",
    "\n",
    "def load_pretrained_weights(model, path):\n",
    "    chk_dict = torch.load(os.path.abspath(path))\n",
    "    model_state_dict = chk_dict[STATE_DICT_KEY] if STATE_DICT_KEY in chk_dict else chk_dict['state_dict']\n",
    "    model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLogger(metaclass=ABCMeta):\n",
    "    def __init__(self, checkpoint_path, metric_key='mean_iou', filename='best_acc_model.pth'):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        if not os.path.exists(self.checkpoint_path):\n",
    "            os.mkdir(self.checkpoint_path)\n",
    "\n",
    "        self.best_metric = 0.\n",
    "        self.metric_key = metric_key\n",
    "        self.filename = filename\n",
    "\n",
    "    def log(self, *args, **kwargs):\n",
    "        current_metric = kwargs[self.metric_key]\n",
    "        if self.best_metric < current_metric:\n",
    "            print(\"Update Best {} Model at {}\".format(self.metric_key, kwargs['epoch']))\n",
    "            self.best_metric = current_metric\n",
    "            save_state_dict(kwargs['state_dict'], self.checkpoint_path, self.filename)\n",
    "    \n",
    "    def complete(self, *args, **kwargs):\n",
    "        save_state_dict(kwargs['state_dict'], self.checkpoint_path, self.filename + '.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggerService(object):\n",
    "    def __init__(self, train_loggers=None, val_loggers=None):\n",
    "        self.train_loggers = train_loggers if train_loggers else []\n",
    "        self.val_loggers = val_loggers if val_loggers else []\n",
    "\n",
    "    def complete(self, log_data):\n",
    "        for logger in self.train_loggers:\n",
    "            logger.complete(**log_data)\n",
    "        for logger in self.val_loggers:\n",
    "            logger.complete(**log_data)\n",
    "\n",
    "    def log_train(self, log_data):\n",
    "        for logger in self.train_loggers:\n",
    "            logger.log(**log_data)\n",
    "\n",
    "    def log_val(self, log_data):\n",
    "        for logger in self.val_loggers:\n",
    "            logger.log(**log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricGraphPrinter(metaclass=ABCMeta):\n",
    "    def __init__(self, writer, key='train_loss', graph_name='Train Loss', group_name='metric'):\n",
    "        self.key = key\n",
    "        self.graph_label = graph_name\n",
    "        self.group_name = group_name\n",
    "        self.writer = writer\n",
    "\n",
    "    def log(self, *args, **kwargs):\n",
    "        if self.key in kwargs:\n",
    "            self.writer.add_scalar(self.group_name + '/' + self.graph_label, kwargs[self.key], kwargs['accum_iter'])\n",
    "        else:\n",
    "            self.writer.add_scalar(self.group_name + '/' + self.graph_label, 0, kwargs['accum_iter'])\n",
    "\n",
    "    def complete(self, *args, **kwargs):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Trainer Class Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTTrainer(metaclass=ABCMeta):\n",
    "    def __init__(self, args, model, train_loader, val_loader, test_loader, export_root):\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "        self.model = model.to(self.device)\n",
    "        self.is_parallel = args.num_gpu > 1\n",
    "        if self.is_parallel:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = self._create_optimizer()\n",
    "        if args.enable_lr_schedule:\n",
    "            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.decay_step, gamma=args.gamma)\n",
    "\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.metric_ks = args.metric_ks\n",
    "        self.best_metric = args.best_metric\n",
    "\n",
    "        self.export_root = export_root\n",
    "        self.writer, self.train_loggers, self.val_loggers = self._create_loggers()\n",
    "\n",
    "        self.logger_service = LoggerService(self.train_loggers, self.val_loggers)\n",
    "        self.log_period_as_iter = args.log_period_as_iter\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    def train(self):\n",
    "        accum_iter = 0\n",
    "        self.validate(0, accum_iter)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            accum_iter = self.train_one_epoch(epoch, accum_iter)\n",
    "            self.validate(epoch, accum_iter)\n",
    "        self.logger_service.complete({\n",
    "            'state_dict': (self._create_state_dict()),\n",
    "        })\n",
    "        self.writer.close()\n",
    "    \n",
    "    def train_one_epoch(self, epoch, accum_iter):\n",
    "        self.model.train()\n",
    "        if self.args.enable_lr_schedule:\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "        average_meter_set = AverageMeterSet()\n",
    "        tqdm_dataloader = tqdm(self.train_loader)\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm_dataloader):\n",
    "            batch_size = batch[0].size(0)\n",
    "            batch = [x.to(self.device) for x in batch]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.calculate_loss(batch)\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            average_meter_set.update('loss', loss.item())\n",
    "            tqdm_dataloader.set_description(\n",
    "                'Epoch {}, loss {:.3f} '.format(epoch+1, average_meter_set['loss'].avg))\n",
    "\n",
    "            accum_iter += batch_size\n",
    "\n",
    "            if self._needs_to_log(accum_iter):\n",
    "                tqdm_dataloader.set_description('Logging to Tensorboard')\n",
    "                log_data = {\n",
    "                    'state_dict': (self._create_state_dict()),\n",
    "                    'epoch': epoch+1,\n",
    "                    'accum_iter': accum_iter,\n",
    "                }\n",
    "                log_data.update(average_meter_set.averages())\n",
    "                self.logger_service.log_train(log_data)\n",
    "\n",
    "        return accum_iter\n",
    "\n",
    "    def validate(self, epoch, accum_iter):\n",
    "        self.model.eval()\n",
    "\n",
    "        average_meter_set = AverageMeterSet()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tqdm_dataloader = tqdm(self.val_loader)\n",
    "            for batch_idx, batch in enumerate(tqdm_dataloader):\n",
    "                batch = [x.to(self.device) for x in batch]\n",
    "\n",
    "                metrics = self.calculate_metrics(batch)\n",
    "\n",
    "                for k, v in metrics.items():\n",
    "                    average_meter_set.update(k, v)\n",
    "                description_metrics = ['NDCG@%d' % k for k in self.metric_ks[:3]] +\\\n",
    "                                      ['Recall@%d' % k for k in self.metric_ks[:3]]\n",
    "                description = 'Val: ' + ', '.join(s + ' {:.3f}' for s in description_metrics)\n",
    "                description = description.replace('NDCG', 'N').replace('Recall', 'R')\n",
    "                description = description.format(*(average_meter_set[k].avg for k in description_metrics))\n",
    "                tqdm_dataloader.set_description(description)\n",
    "\n",
    "            log_data = {\n",
    "                'state_dict': (self._create_state_dict()),\n",
    "                'epoch': epoch+1,\n",
    "                'accum_iter': accum_iter,\n",
    "            }\n",
    "            log_data.update(average_meter_set.averages())\n",
    "            self.logger_service.log_val(log_data)\n",
    "    \n",
    "    def test(self):\n",
    "        print('Test best model with test set!')\n",
    "\n",
    "        best_model = torch.load(os.path.join(self.export_root, 'models', 'best_acc_model.pth')).get('model_state_dict')\n",
    "        self.model.load_state_dict(best_model)\n",
    "        self.model.eval()\n",
    "\n",
    "        average_meter_set = AverageMeterSet()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tqdm_dataloader = tqdm(self.test_loader)\n",
    "            for batch_idx, batch in enumerate(tqdm_dataloader):\n",
    "                batch = [x.to(self.device) for x in batch]\n",
    "\n",
    "                metrics = self.calculate_metrics(batch)\n",
    "\n",
    "                for k, v in metrics.items():\n",
    "                    average_meter_set.update(k, v)\n",
    "                description_metrics = ['NDCG@%d' % k for k in self.metric_ks[:3]] +\\\n",
    "                                      ['Recall@%d' % k for k in self.metric_ks[:3]]\n",
    "                description = 'Val: ' + ', '.join(s + ' {:.3f}' for s in description_metrics)\n",
    "                description = description.replace('NDCG', 'N').replace('Recall', 'R')\n",
    "                description = description.format(*(average_meter_set[k].avg for k in description_metrics))\n",
    "                tqdm_dataloader.set_description(description)\n",
    "\n",
    "            average_metrics = average_meter_set.averages()\n",
    "            with open(os.path.join(self.export_root, 'logs', 'test_metrics.json'), 'w') as f:\n",
    "                json.dump(average_metrics, f, indent=4)\n",
    "            print(average_metrics)\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        return optim.Adam(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    def calculate_loss(self, batch):\n",
    "        seqs, labels = batch\n",
    "        logits = self.model(seqs)  # B x T x V\n",
    "\n",
    "        logits = logits.view(-1, logits.size(-1))  # (B*T) x V\n",
    "        labels = labels.view(-1)  # B*T\n",
    "        loss = self.ce(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def calculate_metrics(self, batch):\n",
    "        seqs, candidates, labels = batch\n",
    "        scores = self.model(seqs)  # B x T x V\n",
    "        scores = scores[:, -1, :]  # B x V\n",
    "        scores = scores.gather(1, candidates)  # B x C\n",
    "\n",
    "        metrics = recalls_and_ndcgs_for_ks(scores, labels, self.metric_ks)\n",
    "        return metrics\n",
    "    \n",
    "    def _create_loggers(self):\n",
    "        root = Path(self.export_root)\n",
    "        writer = SummaryWriter(root.joinpath('logs'))\n",
    "        model_checkpoint = root.joinpath('models')\n",
    "\n",
    "        train_loggers = [\n",
    "            MetricGraphPrinter(writer, key='epoch', graph_name='Epoch', group_name='Train'),\n",
    "            MetricGraphPrinter(writer, key='loss', graph_name='Loss', group_name='Train'),\n",
    "        ]\n",
    "\n",
    "        val_loggers = []\n",
    "        for k in self.metric_ks:\n",
    "            val_loggers.append(\n",
    "                MetricGraphPrinter(writer, key='NDCG@%d' % k, graph_name='NDCG@%d' % k, group_name='Validation'))\n",
    "            val_loggers.append(\n",
    "                MetricGraphPrinter(writer, key='Recall@%d' % k, graph_name='Recall@%d' % k, group_name='Validation'))\n",
    "        val_loggers.append(ModelLogger(model_checkpoint, metric_key=self.best_metric))\n",
    "        return writer, train_loggers, val_loggers\n",
    "\n",
    "    def _create_state_dict(self):\n",
    "        return {\n",
    "            STATE_DICT_KEY: self.model.module.state_dict() if self.is_parallel else self.model.state_dict(),\n",
    "            OPTIMIZER_STATE_DICT_KEY: self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "    def _needs_to_log(self, accum_iter):\n",
    "        return accum_iter % self.log_period_as_iter < self.args.batch_size and accum_iter != 0"
   ]
  },
  {
   "source": [
    "### Intializing the Classes for Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_train(args):\n",
    "\n",
    "    export_root = create_experiment_export_folder(args)\n",
    "    export_experiments_config_as_json(args, export_root)\n",
    "\n",
    "    pp.pprint({k: v for k, v in vars(args).items() if v is not None}, width=1)\n",
    "    return export_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Folder created: c:\\Users\\ngtze\\Desktop\\Bert4Rec\\Bert4Rec\\model\\BT4222_2021-04-10_0\n{'batch_size': 128,\n 'bert_dropout': 0.1,\n 'bert_hidden_units': 256,\n 'bert_mask_prob': 0.15,\n 'bert_max_len': 100,\n 'bert_num_blocks': 2,\n 'bert_num_heads': 4,\n 'best_metric': 'NDCG@10',\n 'decay_step': 25,\n 'device': 'cuda',\n 'device_idx': '0',\n 'enable_lr_schedule': True,\n 'experiment_description': 'BT4222',\n 'experiment_dir': 'model',\n 'gamma': 1.0,\n 'log_period_as_iter': 10000,\n 'lr': 0.001,\n 'metric_ks': [1,\n               5,\n               10,\n               20,\n               50,\n               100],\n 'num_epochs': 100,\n 'num_gpu': 1,\n 'num_items': 2069,\n 'optimizer': 'Adam',\n 'random_seed': 10,\n 'save_folder': '../data/bert_4_rec_data',\n 'test_negative_sample_size': 100,\n 'test_negative_sampler_code': 'random',\n 'test_negative_sampling_seed': 98765,\n 'test_size': 0.19999999999999996,\n 'train_negative_sample_size': 0,\n 'train_negative_sampler_code': 'random',\n 'train_negative_sampling_seed': 0,\n 'train_size': 0.8,\n 'weight_decay': 0}\nNegatives samples exist. Loading.\nNegatives samples exist. Loading.\n"
     ]
    }
   ],
   "source": [
    "export_root = setup_train(args)\n",
    "dataloader = model_Dataloader(args, dataset)\n",
    "train, val, test = dataloader.get_pytorch_dataloaders()\n",
    "model = BERTModel(args)\n",
    "trainer = BERTTrainer(args, model, train, val, test, export_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "@1 0.032, R@5 0.134, R@10 0.194: 100%|| 2/2 [00:00<00:00, 10.02it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 10\n",
      "Epoch 11, loss 6.843 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.024, N@5 0.091, N@10 0.109, R@1 0.024, R@5 0.146, R@10 0.203: 100%|| 2/2 [00:00<00:00, 10.08it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 11\n",
      "Epoch 12, loss 6.735 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.023, N@5 0.085, N@10 0.115, R@1 0.023, R@5 0.142, R@10 0.231: 100%|| 2/2 [00:00<00:00, 10.28it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 12\n",
      "Epoch 13, loss 6.721 : 100%|| 2/2 [00:00<00:00,  4.53it/s]\n",
      "Val: N@1 0.044, N@5 0.104, N@10 0.129, R@1 0.044, R@5 0.162, R@10 0.239: 100%|| 2/2 [00:00<00:00,  9.93it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 13\n",
      "Epoch 14, loss 6.592 : 100%|| 2/2 [00:00<00:00,  4.67it/s]\n",
      "Val: N@1 0.093, N@5 0.130, N@10 0.156, R@1 0.093, R@5 0.162, R@10 0.239: 100%|| 2/2 [00:00<00:00,  9.96it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 14\n",
      "Epoch 15, loss 6.587 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.089, N@5 0.126, N@10 0.150, R@1 0.089, R@5 0.158, R@10 0.231: 100%|| 2/2 [00:00<00:00, 10.02it/s]\n",
      "Epoch 16, loss 6.537 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.093, N@5 0.135, N@10 0.152, R@1 0.093, R@5 0.179, R@10 0.231: 100%|| 2/2 [00:00<00:00, 10.02it/s]\n",
      "Epoch 17, loss 6.462 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.093, N@5 0.143, N@10 0.160, R@1 0.093, R@5 0.195, R@10 0.247: 100%|| 2/2 [00:00<00:00,  9.85it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 17\n",
      "Epoch 18, loss 6.372 : 100%|| 2/2 [00:00<00:00,  4.44it/s]\n",
      "Val: N@1 0.117, N@5 0.158, N@10 0.175, R@1 0.117, R@5 0.199, R@10 0.255: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 18\n",
      "Epoch 19, loss 6.363 : 100%|| 2/2 [00:00<00:00,  4.54it/s]\n",
      "Val: N@1 0.109, N@5 0.157, N@10 0.182, R@1 0.109, R@5 0.199, R@10 0.276: 100%|| 2/2 [00:00<00:00,  9.12it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 19\n",
      "Epoch 20, loss 6.269 : 100%|| 2/2 [00:00<00:00,  4.44it/s]\n",
      "Val: N@1 0.101, N@5 0.159, N@10 0.188, R@1 0.101, R@5 0.203, R@10 0.296: 100%|| 2/2 [00:00<00:00,  8.50it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 20\n",
      "Epoch 21, loss 6.184 : 100%|| 2/2 [00:00<00:00,  4.69it/s]\n",
      "Val: N@1 0.121, N@5 0.173, N@10 0.202, R@1 0.121, R@5 0.219, R@10 0.312: 100%|| 2/2 [00:00<00:00, 10.18it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 21\n",
      "Epoch 22, loss 6.223 : 100%|| 2/2 [00:00<00:00,  4.69it/s]\n",
      "Val: N@1 0.125, N@5 0.175, N@10 0.205, R@1 0.125, R@5 0.222, R@10 0.316: 100%|| 2/2 [00:00<00:00, 10.08it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 22\n",
      "Epoch 23, loss 6.098 : 100%|| 2/2 [00:00<00:00,  4.70it/s]\n",
      "Val: N@1 0.113, N@5 0.188, N@10 0.210, R@1 0.113, R@5 0.259, R@10 0.328: 100%|| 2/2 [00:00<00:00, 10.18it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 23\n",
      "Epoch 24, loss 5.998 : 100%|| 2/2 [00:00<00:00,  4.66it/s]\n",
      "Val: N@1 0.117, N@5 0.197, N@10 0.220, R@1 0.117, R@5 0.275, R@10 0.348: 100%|| 2/2 [00:00<00:00, 10.23it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 24\n",
      "Epoch 25, loss 5.974 : 100%|| 2/2 [00:00<00:00,  4.67it/s]\n",
      "Val: N@1 0.113, N@5 0.195, N@10 0.218, R@1 0.113, R@5 0.268, R@10 0.340: 100%|| 2/2 [00:00<00:00, 10.18it/s]\n",
      "Epoch 26, loss 6.051 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.117, N@5 0.191, N@10 0.215, R@1 0.117, R@5 0.259, R@10 0.332: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 27, loss 5.827 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.109, N@5 0.191, N@10 0.215, R@1 0.109, R@5 0.263, R@10 0.336: 100%|| 2/2 [00:00<00:00,  9.59it/s]\n",
      "Epoch 28, loss 5.780 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.125, N@5 0.203, N@10 0.227, R@1 0.125, R@5 0.271, R@10 0.347: 100%|| 2/2 [00:00<00:00, 10.18it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 28\n",
      "Epoch 29, loss 5.692 : 100%|| 2/2 [00:00<00:00,  4.66it/s]\n",
      "Val: N@1 0.158, N@5 0.220, N@10 0.247, R@1 0.158, R@5 0.283, R@10 0.367: 100%|| 2/2 [00:00<00:00, 10.23it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 29\n",
      "Epoch 30, loss 5.785 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.179, N@5 0.226, N@10 0.258, R@1 0.179, R@5 0.279, R@10 0.380: 100%|| 2/2 [00:00<00:00, 10.18it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 30\n",
      "Epoch 31, loss 5.710 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.175, N@5 0.224, N@10 0.257, R@1 0.175, R@5 0.275, R@10 0.375: 100%|| 2/2 [00:00<00:00, 10.18it/s]\n",
      "Epoch 32, loss 5.664 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.187, N@5 0.236, N@10 0.271, R@1 0.187, R@5 0.287, R@10 0.396: 100%|| 2/2 [00:00<00:00,  9.04it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 32\n",
      "Epoch 33, loss 5.627 : 100%|| 2/2 [00:00<00:00,  4.51it/s]\n",
      "Val: N@1 0.191, N@5 0.242, N@10 0.273, R@1 0.191, R@5 0.299, R@10 0.392: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 33\n",
      "Epoch 34, loss 5.534 : 100%|| 2/2 [00:00<00:00,  4.59it/s]\n",
      "Val: N@1 0.204, N@5 0.251, N@10 0.285, R@1 0.204, R@5 0.298, R@10 0.404: 100%|| 2/2 [00:00<00:00,  9.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 34\n",
      "Epoch 35, loss 5.494 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.204, N@5 0.257, N@10 0.284, R@1 0.204, R@5 0.311, R@10 0.395: 100%|| 2/2 [00:00<00:00,  9.64it/s]\n",
      "Epoch 36, loss 5.422 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.191, N@5 0.255, N@10 0.282, R@1 0.191, R@5 0.311, R@10 0.395: 100%|| 2/2 [00:00<00:00,  9.73it/s]\n",
      "Epoch 37, loss 5.288 : 100%|| 2/2 [00:00<00:00,  4.48it/s]\n",
      "Val: N@1 0.162, N@5 0.243, N@10 0.267, R@1 0.162, R@5 0.311, R@10 0.383: 100%|| 2/2 [00:00<00:00, 10.03it/s]\n",
      "Epoch 38, loss 5.247 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.154, N@5 0.240, N@10 0.271, R@1 0.154, R@5 0.306, R@10 0.404: 100%|| 2/2 [00:00<00:00,  9.69it/s]\n",
      "Epoch 39, loss 5.303 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.186, N@5 0.259, N@10 0.284, R@1 0.186, R@5 0.327, R@10 0.403: 100%|| 2/2 [00:00<00:00, 10.08it/s]\n",
      "Epoch 40, loss 5.159 : 100%|| 2/2 [00:00<00:00,  4.54it/s]\n",
      "Val: N@1 0.203, N@5 0.276, N@10 0.293, R@1 0.203, R@5 0.343, R@10 0.395: 100%|| 2/2 [00:00<00:00,  9.50it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 40\n",
      "Epoch 41, loss 5.184 : 100%|| 2/2 [00:00<00:00,  4.54it/s]\n",
      "Val: N@1 0.178, N@5 0.267, N@10 0.294, R@1 0.178, R@5 0.335, R@10 0.419: 100%|| 2/2 [00:00<00:00,  9.50it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 41\n",
      "Epoch 42, loss 5.169 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.186, N@5 0.272, N@10 0.300, R@1 0.186, R@5 0.343, R@10 0.428: 100%|| 2/2 [00:00<00:00,  8.99it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 42\n",
      "Epoch 43, loss 5.036 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.170, N@5 0.263, N@10 0.288, R@1 0.170, R@5 0.344, R@10 0.424: 100%|| 2/2 [00:00<00:00,  9.69it/s]\n",
      "Epoch 44, loss 5.050 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.183, N@5 0.268, N@10 0.297, R@1 0.183, R@5 0.344, R@10 0.440: 100%|| 2/2 [00:00<00:00,  9.73it/s]\n",
      "Epoch 45, loss 4.994 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.203, N@5 0.269, N@10 0.303, R@1 0.203, R@5 0.327, R@10 0.433: 100%|| 2/2 [00:00<00:00, 10.18it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 45\n",
      "Epoch 46, loss 4.940 : 100%|| 2/2 [00:00<00:00,  4.58it/s]\n",
      "Val: N@1 0.227, N@5 0.281, N@10 0.308, R@1 0.227, R@5 0.336, R@10 0.421: 100%|| 2/2 [00:00<00:00,  9.98it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 46\n",
      "Epoch 47, loss 4.928 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.260, N@5 0.299, N@10 0.327, R@1 0.260, R@5 0.340, R@10 0.428: 100%|| 2/2 [00:00<00:00,  9.12it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 47\n",
      "Epoch 48, loss 4.850 : 100%|| 2/2 [00:00<00:00,  4.58it/s]\n",
      "Val: N@1 0.232, N@5 0.285, N@10 0.318, R@1 0.232, R@5 0.336, R@10 0.440: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 49, loss 4.759 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.220, N@5 0.281, N@10 0.304, R@1 0.220, R@5 0.345, R@10 0.420: 100%|| 2/2 [00:00<00:00,  9.55it/s]\n",
      "Epoch 50, loss 4.759 : 100%|| 2/2 [00:00<00:00,  4.45it/s]\n",
      "Val: N@1 0.203, N@5 0.278, N@10 0.297, R@1 0.203, R@5 0.344, R@10 0.404: 100%|| 2/2 [00:00<00:00,  9.50it/s]\n",
      "Epoch 51, loss 4.707 : 100%|| 2/2 [00:00<00:00,  4.58it/s]\n",
      "Val: N@1 0.211, N@5 0.276, N@10 0.306, R@1 0.211, R@5 0.336, R@10 0.428: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 52, loss 4.725 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.210, N@5 0.283, N@10 0.315, R@1 0.210, R@5 0.352, R@10 0.453: 100%|| 2/2 [00:00<00:00,  9.98it/s]\n",
      "Epoch 53, loss 4.664 : 100%|| 2/2 [00:00<00:00,  4.55it/s]\n",
      "Val: N@1 0.222, N@5 0.289, N@10 0.324, R@1 0.222, R@5 0.356, R@10 0.464: 100%|| 2/2 [00:00<00:00,  9.78it/s]\n",
      "Epoch 54, loss 4.552 : 100%|| 2/2 [00:00<00:00,  4.44it/s]\n",
      "Val: N@1 0.214, N@5 0.296, N@10 0.325, R@1 0.214, R@5 0.375, R@10 0.464: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 55, loss 4.632 : 100%|| 2/2 [00:00<00:00,  4.55it/s]\n",
      "Val: N@1 0.198, N@5 0.275, N@10 0.319, R@1 0.198, R@5 0.340, R@10 0.477: 100%|| 2/2 [00:00<00:00,  9.83it/s]\n",
      "Epoch 56, loss 4.513 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.186, N@5 0.272, N@10 0.317, R@1 0.186, R@5 0.348, R@10 0.485: 100%|| 2/2 [00:00<00:00,  9.59it/s]\n",
      "Epoch 57, loss 4.441 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.223, N@5 0.303, N@10 0.338, R@1 0.223, R@5 0.376, R@10 0.484: 100%|| 2/2 [00:00<00:00, 10.18it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 57\n",
      "Epoch 58, loss 4.473 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.243, N@5 0.311, N@10 0.345, R@1 0.243, R@5 0.376, R@10 0.484: 100%|| 2/2 [00:00<00:00,  9.93it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 58\n",
      "Epoch 59, loss 4.374 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.239, N@5 0.309, N@10 0.340, R@1 0.239, R@5 0.376, R@10 0.472: 100%|| 2/2 [00:00<00:00,  9.41it/s]\n",
      "Epoch 60, loss 4.285 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.255, N@5 0.316, N@10 0.348, R@1 0.255, R@5 0.375, R@10 0.476: 100%|| 2/2 [00:00<00:00, 10.23it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 60\n",
      "Epoch 61, loss 4.320 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.251, N@5 0.311, N@10 0.347, R@1 0.251, R@5 0.368, R@10 0.480: 100%|| 2/2 [00:00<00:00,  9.03it/s]\n",
      "Epoch 62, loss 4.272 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.247, N@5 0.307, N@10 0.348, R@1 0.247, R@5 0.360, R@10 0.488: 100%|| 2/2 [00:00<00:00,  9.88it/s]\n",
      "Epoch 63, loss 4.257 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.243, N@5 0.312, N@10 0.349, R@1 0.243, R@5 0.375, R@10 0.488: 100%|| 2/2 [00:00<00:00,  9.88it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 63\n",
      "Epoch 64, loss 4.204 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.244, N@5 0.321, N@10 0.352, R@1 0.244, R@5 0.391, R@10 0.488: 100%|| 2/2 [00:00<00:00,  9.59it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 64\n",
      "Epoch 65, loss 4.195 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.248, N@5 0.329, N@10 0.355, R@1 0.248, R@5 0.403, R@10 0.480: 100%|| 2/2 [00:00<00:00,  9.16it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 65\n",
      "Epoch 66, loss 4.183 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.255, N@5 0.341, N@10 0.363, R@1 0.255, R@5 0.424, R@10 0.493: 100%|| 2/2 [00:00<00:00,  9.11it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 66\n",
      "Epoch 67, loss 4.091 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.247, N@5 0.338, N@10 0.365, R@1 0.247, R@5 0.419, R@10 0.501: 100%|| 2/2 [00:00<00:00,  9.37it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 67\n",
      "Epoch 68, loss 4.050 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.252, N@5 0.335, N@10 0.366, R@1 0.252, R@5 0.408, R@10 0.501: 100%|| 2/2 [00:00<00:00, 10.08it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 68\n",
      "Epoch 69, loss 3.992 : 100%|| 2/2 [00:00<00:00,  4.66it/s]\n",
      "Val: N@1 0.239, N@5 0.329, N@10 0.358, R@1 0.239, R@5 0.412, R@10 0.504: 100%|| 2/2 [00:00<00:00, 10.30it/s]\n",
      "Epoch 70, loss 4.075 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.243, N@5 0.343, N@10 0.371, R@1 0.243, R@5 0.433, R@10 0.521: 100%|| 2/2 [00:00<00:00, 10.03it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 70\n",
      "Epoch 71, loss 3.925 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.252, N@5 0.341, N@10 0.369, R@1 0.252, R@5 0.424, R@10 0.508: 100%|| 2/2 [00:00<00:00,  9.73it/s]\n",
      "Epoch 72, loss 4.022 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.259, N@5 0.339, N@10 0.363, R@1 0.259, R@5 0.420, R@10 0.488: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 73, loss 3.903 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.284, N@5 0.366, N@10 0.384, R@1 0.284, R@5 0.444, R@10 0.501: 100%|| 2/2 [00:00<00:00,  9.78it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 73\n",
      "Epoch 74, loss 3.885 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.308, N@5 0.382, N@10 0.399, R@1 0.308, R@5 0.457, R@10 0.508: 100%|| 2/2 [00:00<00:00,  9.82it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 74\n",
      "Epoch 75, loss 3.866 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.324, N@5 0.384, N@10 0.405, R@1 0.324, R@5 0.440, R@10 0.504: 100%|| 2/2 [00:00<00:00,  8.86it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 75\n",
      "Epoch 76, loss 3.776 : 100%|| 2/2 [00:00<00:00,  4.53it/s]\n",
      "Val: N@1 0.324, N@5 0.385, N@10 0.410, R@1 0.324, R@5 0.436, R@10 0.513: 100%|| 2/2 [00:00<00:00,  9.24it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 76\n",
      "Epoch 77, loss 3.736 : 100%|| 2/2 [00:00<00:00,  4.57it/s]\n",
      "Val: N@1 0.341, N@5 0.397, N@10 0.419, R@1 0.341, R@5 0.449, R@10 0.517: 100%|| 2/2 [00:00<00:00,  9.64it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 77\n",
      "Epoch 78, loss 3.696 : 100%|| 2/2 [00:00<00:00,  4.52it/s]\n",
      "Val: N@1 0.308, N@5 0.383, N@10 0.408, R@1 0.308, R@5 0.449, R@10 0.528: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 79, loss 3.620 : 100%|| 2/2 [00:00<00:00,  4.50it/s]\n",
      "Val: N@1 0.284, N@5 0.371, N@10 0.400, R@1 0.284, R@5 0.448, R@10 0.541: 100%|| 2/2 [00:00<00:00, 10.08it/s]\n",
      "Epoch 80, loss 3.672 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.267, N@5 0.367, N@10 0.393, R@1 0.267, R@5 0.457, R@10 0.537: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 81, loss 3.575 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.271, N@5 0.365, N@10 0.394, R@1 0.271, R@5 0.453, R@10 0.541: 100%|| 2/2 [00:00<00:00, 10.28it/s]\n",
      "Epoch 82, loss 3.617 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.296, N@5 0.375, N@10 0.403, R@1 0.296, R@5 0.452, R@10 0.541: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 83, loss 3.546 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.311, N@5 0.382, N@10 0.414, R@1 0.311, R@5 0.448, R@10 0.548: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 84, loss 3.556 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.323, N@5 0.389, N@10 0.415, R@1 0.323, R@5 0.448, R@10 0.528: 100%|| 2/2 [00:00<00:00, 10.33it/s]\n",
      "Epoch 85, loss 3.502 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.311, N@5 0.377, N@10 0.409, R@1 0.311, R@5 0.432, R@10 0.532: 100%|| 2/2 [00:00<00:00,  9.78it/s]\n",
      "Epoch 86, loss 3.519 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.302, N@5 0.368, N@10 0.397, R@1 0.302, R@5 0.423, R@10 0.516: 100%|| 2/2 [00:00<00:00,  9.93it/s]\n",
      "Epoch 87, loss 3.487 : 100%|| 2/2 [00:00<00:00,  4.56it/s]\n",
      "Val: N@1 0.295, N@5 0.372, N@10 0.402, R@1 0.295, R@5 0.435, R@10 0.528: 100%|| 2/2 [00:00<00:00, 10.03it/s]\n",
      "Epoch 88, loss 3.303 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.295, N@5 0.376, N@10 0.405, R@1 0.295, R@5 0.444, R@10 0.532: 100%|| 2/2 [00:00<00:00, 10.33it/s]\n",
      "Epoch 89, loss 3.392 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.291, N@5 0.371, N@10 0.402, R@1 0.291, R@5 0.439, R@10 0.536: 100%|| 2/2 [00:00<00:00,  9.03it/s]\n",
      "Epoch 90, loss 3.357 : 100%|| 2/2 [00:00<00:00,  4.44it/s]\n",
      "Val: N@1 0.286, N@5 0.373, N@10 0.399, R@1 0.286, R@5 0.452, R@10 0.532: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 91, loss 3.366 : 100%|| 2/2 [00:00<00:00,  4.57it/s]\n",
      "Val: N@1 0.295, N@5 0.378, N@10 0.404, R@1 0.295, R@5 0.452, R@10 0.532: 100%|| 2/2 [00:00<00:00, 10.03it/s]\n",
      "Epoch 92, loss 3.368 : 100%|| 2/2 [00:00<00:00,  4.56it/s]\n",
      "Val: N@1 0.298, N@5 0.383, N@10 0.411, R@1 0.298, R@5 0.452, R@10 0.536: 100%|| 2/2 [00:00<00:00,  9.59it/s]\n",
      "Epoch 93, loss 3.280 : 100%|| 2/2 [00:00<00:00,  4.62it/s]\n",
      "Val: N@1 0.315, N@5 0.393, N@10 0.420, R@1 0.315, R@5 0.455, R@10 0.540: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 93\n",
      "Epoch 94, loss 3.226 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.339, N@5 0.403, N@10 0.425, R@1 0.339, R@5 0.459, R@10 0.528: 100%|| 2/2 [00:00<00:00, 10.23it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 94\n",
      "Epoch 95, loss 3.237 : 100%|| 2/2 [00:00<00:00,  4.59it/s]\n",
      "Val: N@1 0.367, N@5 0.420, N@10 0.440, R@1 0.367, R@5 0.468, R@10 0.528: 100%|| 2/2 [00:00<00:00, 10.39it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Update Best NDCG@10 Model at 95\n",
      "Epoch 96, loss 3.231 : 100%|| 2/2 [00:00<00:00,  4.59it/s]\n",
      "Val: N@1 0.364, N@5 0.421, N@10 0.439, R@1 0.364, R@5 0.472, R@10 0.524: 100%|| 2/2 [00:00<00:00, 10.28it/s]\n",
      "Epoch 97, loss 3.177 : 100%|| 2/2 [00:00<00:00,  4.65it/s]\n",
      "Val: N@1 0.355, N@5 0.410, N@10 0.430, R@1 0.355, R@5 0.463, R@10 0.524: 100%|| 2/2 [00:00<00:00, 10.33it/s]\n",
      "Epoch 98, loss 3.142 : 100%|| 2/2 [00:00<00:00,  4.64it/s]\n",
      "Val: N@1 0.339, N@5 0.405, N@10 0.425, R@1 0.339, R@5 0.468, R@10 0.528: 100%|| 2/2 [00:00<00:00, 10.13it/s]\n",
      "Epoch 99, loss 3.117 : 100%|| 2/2 [00:00<00:00,  4.63it/s]\n",
      "Val: N@1 0.347, N@5 0.416, N@10 0.439, R@1 0.347, R@5 0.476, R@10 0.549: 100%|| 2/2 [00:00<00:00,  9.78it/s]\n",
      "Epoch 100, loss 3.092 : 100%|| 2/2 [00:00<00:00,  4.61it/s]\n",
      "Val: N@1 0.360, N@5 0.419, N@10 0.440, R@1 0.360, R@5 0.472, R@10 0.540: 100%|| 2/2 [00:00<00:00, 10.08it/s]\n",
      "Update Best NDCG@10 Model at 100\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test best model with test set!\n",
      "Val: N@1 0.157, N@5 0.217, N@10 0.251, R@1 0.157, R@5 0.270, R@10 0.375: 100%|| 2/2 [00:00<00:00,  9.51it/s]{'Recall@100': 1.0, 'NDCG@100': 0.36983881890773773, 'Recall@50': 0.6609375178813934, 'NDCG@50': 0.3150499314069748, 'Recall@20': 0.5127604305744171, 'NDCG@20': 0.28583210706710815, 'Recall@10': 0.3752604275941849, 'NDCG@10': 0.25090668350458145, 'Recall@5': 0.26953125, 'NDCG@5': 0.21746379882097244, 'Recall@1': 0.15651042014360428, 'NDCG@1': 0.15651042014360428}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  }
 ]
}