{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xkX0DQi73Lps"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.constraints import maxnorm\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,  Flatten, Input\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import metrics\n",
    "from keras.metrics import AUC\n",
    "from keras.metrics import Precision\n",
    "from keras.metrics import Recall\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import sklearn\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VphfsiPaUkpK",
    "outputId": "3c765e78-e1a7-4dac-a29c-043410cf33a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GFdIm29KyhC"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XXf0X9Vv3bJ0"
   },
   "outputs": [],
   "source": [
    "X_train_trf1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/X_train_trf1_v3.csv')\n",
    "X_val_trf1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/X_val_trf1_v3.csv')\n",
    "X_test_trf1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/X_test_trf1_v3.csv')\n",
    "\n",
    "#Drop rating_month and rating_year columns\n",
    "#Drop primaryTitle, userId_ori, movieId_ori\n",
    "\n",
    "extracted_cols = X_test_trf1.loc[:,['primaryTitle', 'movieId_ori','userId_ori']]\n",
    "\n",
    "def dropRatingDate(df):\n",
    "  cols = [0, 1, 2, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 42]\n",
    "  return df.drop(df.columns[cols], axis=1)\n",
    "\n",
    "X_train_trf1 = dropRatingDate(X_train_trf1)\n",
    "X_test_trf1 = dropRatingDate(X_test_trf1)\n",
    "X_val_trf1 = dropRatingDate(X_val_trf1)\n",
    "\n",
    "columns_list = X_train_trf1.columns.tolist()\n",
    "\n",
    "X_train_trf1 = X_train_trf1.to_numpy()\n",
    "X_val_trf1 = X_val_trf1.to_numpy()\n",
    "X_test_trf1 = X_test_trf1.to_numpy() \n",
    "\n",
    "y_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/y_train_xg_v3.csv').to_numpy(dtype = 'int')\n",
    "y_val= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/y_val_xg_v3.csv').to_numpy(dtype = 'int')\n",
    "y_test= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/y_test_xg_v3.csv').to_numpy(dtype = 'int')\n",
    "y_test_ori = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kHxPvGPH3jYc"
   },
   "outputs": [],
   "source": [
    "#Convert 9bins to 2bins\n",
    "def nineToTwoBins(y_dataset):\n",
    "    for i in range(0, len(y_dataset)): \n",
    "        if y_dataset[i] < 6:\n",
    "            y_dataset[i] = 0\n",
    "        else:\n",
    "            y_dataset[i] = 1\n",
    "    return y_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRjBnCN03lu5",
    "outputId": "753c3d7c-b20a-4f27-c36e-257b7b8ce871"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nineToTwoBins(y_train)\n",
    "nineToTwoBins(y_val)\n",
    "nineToTwoBins(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vvgWQhOK3zo0"
   },
   "outputs": [],
   "source": [
    "training_data = {'X_train':X_train_trf1,'Y_train':y_train,\n",
    "                'X_val': X_val_trf1,'Y_val':y_val,\n",
    "                'X_test': X_test_trf1 ,'Y_test':y_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjVf5k-4K6HH"
   },
   "source": [
    "# Stacking and Loading Optimized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y54ILB2BKQsu"
   },
   "source": [
    "**Build Model Function based on Bayesian Optimized Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Mp6oh9w_9JDL"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "\n",
    "  # Define input layer, and first hidden layer\n",
    "  # neurons for first hidden layer\n",
    "  model.add(Dense(970, input_dim=27, activation='relu')\n",
    "  )\n",
    "\n",
    "  # Define 2nd hidden (Dense) layers onward\n",
    "  # Tune layers, neurons\n",
    "  for i, neuron in enumerate([970, 970, 20, 970, 970,\n",
    "                              970, 970, 970, 970, 970,\n",
    "                              970, 970], start = 2):\n",
    "    model.add(Dense(units=neuron,\n",
    "                    activation='relu', \n",
    "                    name=f'Hidden{i}'))\n",
    "    # Add drop out layers to 5th hidden layer\n",
    "    if i == 4:\n",
    "      model.add(Dropout(rate=0.25)\n",
    "      )\n",
    "\n",
    "  # Define output layer\n",
    "  model.add(Dense(1, activation='sigmoid')\n",
    "  )\n",
    "\n",
    "  # Compile model\n",
    "  # Tune learning rate at optimizer\n",
    "  model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=[AUC(),Precision(), Recall()]\n",
    "                  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDgoJyRkKiO7"
   },
   "source": [
    "**XGBClassifier Based on GridSearch Tuned Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qg_faBp34Z9c",
    "outputId": "8056e9a6-ea8c-4478-f337-e9c566cf326b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "466/466 [==============================] - 10s 12ms/step - loss: 0.6786 - auc: 0.5845 - precision: 0.5630 - recall: 0.4281\n",
      "Epoch 2/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6512 - auc: 0.6625 - precision: 0.6181 - recall: 0.5940\n",
      "Epoch 3/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6440 - auc: 0.6757 - precision: 0.6264 - recall: 0.6107\n",
      "Epoch 4/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6396 - auc: 0.6846 - precision: 0.6327 - recall: 0.6083\n",
      "Epoch 5/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6346 - auc: 0.6933 - precision: 0.6367 - recall: 0.6191\n",
      "Epoch 6/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6307 - auc: 0.6987 - precision: 0.6379 - recall: 0.6221\n",
      "Epoch 7/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6266 - auc: 0.7044 - precision: 0.6406 - recall: 0.6374\n",
      "Epoch 8/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6245 - auc: 0.7074 - precision: 0.6439 - recall: 0.6378\n",
      "Epoch 9/80\n",
      "466/466 [==============================] - 6s 13ms/step - loss: 0.6230 - auc: 0.7097 - precision: 0.6521 - recall: 0.6273\n",
      "Epoch 10/80\n",
      "466/466 [==============================] - 6s 13ms/step - loss: 0.6230 - auc: 0.7092 - precision: 0.6474 - recall: 0.6336\n",
      "Epoch 11/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6190 - auc: 0.7148 - precision: 0.6511 - recall: 0.6418\n",
      "Epoch 12/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6190 - auc: 0.7148 - precision: 0.6517 - recall: 0.6366\n",
      "Epoch 13/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6182 - auc: 0.7154 - precision: 0.6497 - recall: 0.6389\n",
      "Epoch 14/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6156 - auc: 0.7192 - precision: 0.6540 - recall: 0.6451\n",
      "Epoch 15/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6150 - auc: 0.7197 - precision: 0.6518 - recall: 0.6511\n",
      "Epoch 16/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6143 - auc: 0.7210 - precision: 0.6538 - recall: 0.6505\n",
      "Epoch 17/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6142 - auc: 0.7207 - precision: 0.6530 - recall: 0.6523\n",
      "Epoch 18/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6142 - auc: 0.7212 - precision: 0.6525 - recall: 0.6589\n",
      "Epoch 19/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6131 - auc: 0.7224 - precision: 0.6554 - recall: 0.6469\n",
      "Epoch 20/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6122 - auc: 0.7233 - precision: 0.6543 - recall: 0.6578\n",
      "Epoch 21/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6123 - auc: 0.7233 - precision: 0.6562 - recall: 0.6489\n",
      "Epoch 22/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6113 - auc: 0.7248 - precision: 0.6557 - recall: 0.6548\n",
      "Epoch 23/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6085 - auc: 0.7284 - precision: 0.6615 - recall: 0.6517\n",
      "Epoch 24/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6080 - auc: 0.7289 - precision: 0.6596 - recall: 0.6575\n",
      "Epoch 25/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6093 - auc: 0.7276 - precision: 0.6621 - recall: 0.6517\n",
      "Epoch 26/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6083 - auc: 0.7285 - precision: 0.6586 - recall: 0.6595\n",
      "Epoch 27/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6078 - auc: 0.7289 - precision: 0.6558 - recall: 0.6688\n",
      "Epoch 28/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6073 - auc: 0.7296 - precision: 0.6600 - recall: 0.6650\n",
      "Epoch 29/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6062 - auc: 0.7311 - precision: 0.6603 - recall: 0.6614\n",
      "Epoch 30/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6068 - auc: 0.7300 - precision: 0.6582 - recall: 0.6606\n",
      "Epoch 31/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6047 - auc: 0.7328 - precision: 0.6632 - recall: 0.6589\n",
      "Epoch 32/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6050 - auc: 0.7325 - precision: 0.6629 - recall: 0.6574\n",
      "Epoch 33/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6037 - auc: 0.7340 - precision: 0.6647 - recall: 0.6590\n",
      "Epoch 34/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6034 - auc: 0.7345 - precision: 0.6630 - recall: 0.6639\n",
      "Epoch 35/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6037 - auc: 0.7338 - precision: 0.6641 - recall: 0.6584\n",
      "Epoch 36/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6033 - auc: 0.7343 - precision: 0.6608 - recall: 0.6676\n",
      "Epoch 37/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6020 - auc: 0.7356 - precision: 0.6621 - recall: 0.6676\n",
      "Epoch 38/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6022 - auc: 0.7360 - precision: 0.6648 - recall: 0.6719\n",
      "Epoch 39/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6004 - auc: 0.7381 - precision: 0.6655 - recall: 0.6632\n",
      "Epoch 40/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6011 - auc: 0.7370 - precision: 0.6650 - recall: 0.6647\n",
      "Epoch 41/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6004 - auc: 0.7380 - precision: 0.6658 - recall: 0.6698\n",
      "Epoch 42/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.6002 - auc: 0.7381 - precision: 0.6670 - recall: 0.6613\n",
      "Epoch 43/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5976 - auc: 0.7414 - precision: 0.6679 - recall: 0.6740\n",
      "Epoch 44/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5977 - auc: 0.7413 - precision: 0.6640 - recall: 0.6748\n",
      "Epoch 45/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5964 - auc: 0.7426 - precision: 0.6681 - recall: 0.6711\n",
      "Epoch 46/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5959 - auc: 0.7432 - precision: 0.6704 - recall: 0.6612\n",
      "Epoch 47/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5977 - auc: 0.7411 - precision: 0.6670 - recall: 0.6690\n",
      "Epoch 48/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5964 - auc: 0.7422 - precision: 0.6723 - recall: 0.6604\n",
      "Epoch 49/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5947 - auc: 0.7442 - precision: 0.6703 - recall: 0.6699\n",
      "Epoch 50/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5957 - auc: 0.7432 - precision: 0.6726 - recall: 0.6652\n",
      "Epoch 51/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5946 - auc: 0.7440 - precision: 0.6714 - recall: 0.6651\n",
      "Epoch 52/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5923 - auc: 0.7473 - precision: 0.6729 - recall: 0.6753\n",
      "Epoch 53/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5925 - auc: 0.7467 - precision: 0.6718 - recall: 0.6732\n",
      "Epoch 54/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5931 - auc: 0.7463 - precision: 0.6747 - recall: 0.6635\n",
      "Epoch 55/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5930 - auc: 0.7463 - precision: 0.6741 - recall: 0.6685\n",
      "Epoch 56/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5925 - auc: 0.7468 - precision: 0.6736 - recall: 0.6669\n",
      "Epoch 57/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5901 - auc: 0.7492 - precision: 0.6752 - recall: 0.6705\n",
      "Epoch 58/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5906 - auc: 0.7488 - precision: 0.6755 - recall: 0.6643\n",
      "Epoch 59/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5902 - auc: 0.7490 - precision: 0.6723 - recall: 0.6766\n",
      "Epoch 60/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5889 - auc: 0.7505 - precision: 0.6751 - recall: 0.6722\n",
      "Epoch 61/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5892 - auc: 0.7502 - precision: 0.6737 - recall: 0.6782\n",
      "Epoch 62/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5888 - auc: 0.7502 - precision: 0.6726 - recall: 0.6756\n",
      "Epoch 63/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5873 - auc: 0.7526 - precision: 0.6771 - recall: 0.6747\n",
      "Epoch 64/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5884 - auc: 0.7516 - precision: 0.6761 - recall: 0.6789\n",
      "Epoch 65/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5846 - auc: 0.7552 - precision: 0.6772 - recall: 0.6826\n",
      "Epoch 66/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5860 - auc: 0.7536 - precision: 0.6783 - recall: 0.6800\n",
      "Epoch 67/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5850 - auc: 0.7549 - precision: 0.6781 - recall: 0.6726\n",
      "Epoch 68/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5848 - auc: 0.7546 - precision: 0.6786 - recall: 0.6725\n",
      "Epoch 69/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5847 - auc: 0.7556 - precision: 0.6785 - recall: 0.6804\n",
      "Epoch 70/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5843 - auc: 0.7557 - precision: 0.6787 - recall: 0.6855\n",
      "Epoch 71/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5816 - auc: 0.7587 - precision: 0.6824 - recall: 0.6816\n",
      "Epoch 72/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5832 - auc: 0.7562 - precision: 0.6782 - recall: 0.6771\n",
      "Epoch 73/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5801 - auc: 0.7601 - precision: 0.6809 - recall: 0.6887\n",
      "Epoch 74/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5809 - auc: 0.7593 - precision: 0.6814 - recall: 0.6905\n",
      "Epoch 75/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5810 - auc: 0.7591 - precision: 0.6804 - recall: 0.6903\n",
      "Epoch 76/80\n",
      "466/466 [==============================] - 6s 13ms/step - loss: 0.5806 - auc: 0.7592 - precision: 0.6806 - recall: 0.6875\n",
      "Epoch 77/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5801 - auc: 0.7598 - precision: 0.6789 - recall: 0.6905\n",
      "Epoch 78/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5788 - auc: 0.7608 - precision: 0.6825 - recall: 0.6855\n",
      "Epoch 79/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5787 - auc: 0.7612 - precision: 0.6845 - recall: 0.6816\n",
      "Epoch 80/80\n",
      "466/466 [==============================] - 6s 12ms/step - loss: 0.5781 - auc: 0.7619 - precision: 0.6829 - recall: 0.6889\n",
      "Epoch 1/80\n",
      "373/373 [==============================] - 6s 13ms/step - loss: 0.6793 - auc_1: 0.5873 - precision_1: 0.5566 - recall_1: 0.4537\n",
      "Epoch 2/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6572 - auc_1: 0.6523 - precision_1: 0.6073 - recall_1: 0.5999\n",
      "Epoch 3/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6533 - auc_1: 0.6601 - precision_1: 0.6188 - recall_1: 0.5716\n",
      "Epoch 4/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6432 - auc_1: 0.6783 - precision_1: 0.6312 - recall_1: 0.5951\n",
      "Epoch 5/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6439 - auc_1: 0.6770 - precision_1: 0.6272 - recall_1: 0.6029\n",
      "Epoch 6/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6391 - auc_1: 0.6854 - precision_1: 0.6347 - recall_1: 0.6082\n",
      "Epoch 7/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6365 - auc_1: 0.6888 - precision_1: 0.6358 - recall_1: 0.6005\n",
      "Epoch 8/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6340 - auc_1: 0.6927 - precision_1: 0.6363 - recall_1: 0.6209\n",
      "Epoch 9/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6305 - auc_1: 0.6982 - precision_1: 0.6445 - recall_1: 0.6167\n",
      "Epoch 10/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6265 - auc_1: 0.7041 - precision_1: 0.6446 - recall_1: 0.6282\n",
      "Epoch 11/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6263 - auc_1: 0.7047 - precision_1: 0.6448 - recall_1: 0.6355\n",
      "Epoch 12/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6233 - auc_1: 0.7090 - precision_1: 0.6483 - recall_1: 0.6284\n",
      "Epoch 13/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6244 - auc_1: 0.7075 - precision_1: 0.6454 - recall_1: 0.6449\n",
      "Epoch 14/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6219 - auc_1: 0.7108 - precision_1: 0.6444 - recall_1: 0.6454\n",
      "Epoch 15/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6198 - auc_1: 0.7139 - precision_1: 0.6489 - recall_1: 0.6441\n",
      "Epoch 16/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6177 - auc_1: 0.7172 - precision_1: 0.6530 - recall_1: 0.6507\n",
      "Epoch 17/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6184 - auc_1: 0.7153 - precision_1: 0.6486 - recall_1: 0.6573\n",
      "Epoch 18/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6153 - auc_1: 0.7200 - precision_1: 0.6520 - recall_1: 0.6531\n",
      "Epoch 19/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6146 - auc_1: 0.7207 - precision_1: 0.6590 - recall_1: 0.6422\n",
      "Epoch 20/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6168 - auc_1: 0.7183 - precision_1: 0.6519 - recall_1: 0.6557\n",
      "Epoch 21/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6160 - auc_1: 0.7186 - precision_1: 0.6511 - recall_1: 0.6491\n",
      "Epoch 22/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6141 - auc_1: 0.7213 - precision_1: 0.6552 - recall_1: 0.6470\n",
      "Epoch 23/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6135 - auc_1: 0.7222 - precision_1: 0.6520 - recall_1: 0.6501\n",
      "Epoch 24/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6136 - auc_1: 0.7219 - precision_1: 0.6560 - recall_1: 0.6491\n",
      "Epoch 25/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6110 - auc_1: 0.7255 - precision_1: 0.6567 - recall_1: 0.6592\n",
      "Epoch 26/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6094 - auc_1: 0.7276 - precision_1: 0.6587 - recall_1: 0.6559\n",
      "Epoch 27/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6112 - auc_1: 0.7248 - precision_1: 0.6602 - recall_1: 0.6406\n",
      "Epoch 28/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6102 - auc_1: 0.7268 - precision_1: 0.6594 - recall_1: 0.6540\n",
      "Epoch 29/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6078 - auc_1: 0.7295 - precision_1: 0.6621 - recall_1: 0.6444\n",
      "Epoch 30/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6086 - auc_1: 0.7285 - precision_1: 0.6624 - recall_1: 0.6472\n",
      "Epoch 31/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6073 - auc_1: 0.7302 - precision_1: 0.6649 - recall_1: 0.6421\n",
      "Epoch 32/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6067 - auc_1: 0.7307 - precision_1: 0.6611 - recall_1: 0.6533\n",
      "Epoch 33/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6074 - auc_1: 0.7300 - precision_1: 0.6616 - recall_1: 0.6601\n",
      "Epoch 34/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6053 - auc_1: 0.7323 - precision_1: 0.6603 - recall_1: 0.6632\n",
      "Epoch 35/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6058 - auc_1: 0.7320 - precision_1: 0.6596 - recall_1: 0.6665\n",
      "Epoch 36/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6053 - auc_1: 0.7323 - precision_1: 0.6603 - recall_1: 0.6620\n",
      "Epoch 37/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6054 - auc_1: 0.7324 - precision_1: 0.6626 - recall_1: 0.6580\n",
      "Epoch 38/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6048 - auc_1: 0.7331 - precision_1: 0.6614 - recall_1: 0.6647\n",
      "Epoch 39/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6018 - auc_1: 0.7369 - precision_1: 0.6656 - recall_1: 0.6628\n",
      "Epoch 40/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6000 - auc_1: 0.7389 - precision_1: 0.6662 - recall_1: 0.6666\n",
      "Epoch 41/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6034 - auc_1: 0.7342 - precision_1: 0.6618 - recall_1: 0.6581\n",
      "Epoch 42/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5999 - auc_1: 0.7390 - precision_1: 0.6663 - recall_1: 0.6603\n",
      "Epoch 43/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6009 - auc_1: 0.7373 - precision_1: 0.6670 - recall_1: 0.6515\n",
      "Epoch 44/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6007 - auc_1: 0.7381 - precision_1: 0.6663 - recall_1: 0.6605\n",
      "Epoch 45/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6000 - auc_1: 0.7386 - precision_1: 0.6710 - recall_1: 0.6539\n",
      "Epoch 46/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5990 - auc_1: 0.7400 - precision_1: 0.6721 - recall_1: 0.6559\n",
      "Epoch 47/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5977 - auc_1: 0.7417 - precision_1: 0.6743 - recall_1: 0.6552\n",
      "Epoch 48/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5983 - auc_1: 0.7402 - precision_1: 0.6689 - recall_1: 0.6655\n",
      "Epoch 49/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5951 - auc_1: 0.7443 - precision_1: 0.6699 - recall_1: 0.6677\n",
      "Epoch 50/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5964 - auc_1: 0.7427 - precision_1: 0.6711 - recall_1: 0.6578\n",
      "Epoch 51/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5944 - auc_1: 0.7452 - precision_1: 0.6763 - recall_1: 0.6496\n",
      "Epoch 52/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5958 - auc_1: 0.7437 - precision_1: 0.6734 - recall_1: 0.6569\n",
      "Epoch 53/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5970 - auc_1: 0.7419 - precision_1: 0.6719 - recall_1: 0.6537\n",
      "Epoch 54/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5948 - auc_1: 0.7444 - precision_1: 0.6737 - recall_1: 0.6522\n",
      "Epoch 55/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5949 - auc_1: 0.7446 - precision_1: 0.6737 - recall_1: 0.6598\n",
      "Epoch 56/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5919 - auc_1: 0.7481 - precision_1: 0.6771 - recall_1: 0.6592\n",
      "Epoch 57/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5933 - auc_1: 0.7460 - precision_1: 0.6757 - recall_1: 0.6532\n",
      "Epoch 58/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5925 - auc_1: 0.7473 - precision_1: 0.6769 - recall_1: 0.6644\n",
      "Epoch 59/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5932 - auc_1: 0.7465 - precision_1: 0.6772 - recall_1: 0.6586\n",
      "Epoch 60/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5902 - auc_1: 0.7499 - precision_1: 0.6788 - recall_1: 0.6614\n",
      "Epoch 61/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5925 - auc_1: 0.7471 - precision_1: 0.6765 - recall_1: 0.6604\n",
      "Epoch 62/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5904 - auc_1: 0.7494 - precision_1: 0.6800 - recall_1: 0.6610\n",
      "Epoch 63/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5899 - auc_1: 0.7500 - precision_1: 0.6790 - recall_1: 0.6580\n",
      "Epoch 64/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5872 - auc_1: 0.7535 - precision_1: 0.6822 - recall_1: 0.6658\n",
      "Epoch 65/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5903 - auc_1: 0.7490 - precision_1: 0.6778 - recall_1: 0.6613\n",
      "Epoch 66/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5871 - auc_1: 0.7527 - precision_1: 0.6798 - recall_1: 0.6658\n",
      "Epoch 67/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5862 - auc_1: 0.7542 - precision_1: 0.6814 - recall_1: 0.6661\n",
      "Epoch 68/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5860 - auc_1: 0.7539 - precision_1: 0.6758 - recall_1: 0.6781\n",
      "Epoch 69/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5847 - auc_1: 0.7552 - precision_1: 0.6781 - recall_1: 0.6759\n",
      "Epoch 70/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5849 - auc_1: 0.7554 - precision_1: 0.6813 - recall_1: 0.6694\n",
      "Epoch 71/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5854 - auc_1: 0.7549 - precision_1: 0.6800 - recall_1: 0.6757\n",
      "Epoch 72/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5826 - auc_1: 0.7577 - precision_1: 0.6805 - recall_1: 0.6745\n",
      "Epoch 73/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5827 - auc_1: 0.7578 - precision_1: 0.6847 - recall_1: 0.6715\n",
      "Epoch 74/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5839 - auc_1: 0.7564 - precision_1: 0.6822 - recall_1: 0.6769\n",
      "Epoch 75/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5833 - auc_1: 0.7569 - precision_1: 0.6822 - recall_1: 0.6680\n",
      "Epoch 76/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5833 - auc_1: 0.7570 - precision_1: 0.6838 - recall_1: 0.6699\n",
      "Epoch 77/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5832 - auc_1: 0.7569 - precision_1: 0.6801 - recall_1: 0.6796\n",
      "Epoch 78/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5824 - auc_1: 0.7578 - precision_1: 0.6833 - recall_1: 0.6715\n",
      "Epoch 79/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5803 - auc_1: 0.7600 - precision_1: 0.6852 - recall_1: 0.6681\n",
      "Epoch 80/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5790 - auc_1: 0.7610 - precision_1: 0.6847 - recall_1: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "373/373 [==============================] - 6s 12ms/step - loss: 0.6929 - auc_2: 0.5005 - precision_2: 0.4834 - recall_2: 0.0658\n",
      "Epoch 2/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4984 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 3/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.5011 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 4/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4986 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 5/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6929 - auc_2: 0.5001 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 6/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4972 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 7/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6929 - auc_2: 0.5005 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 8/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 9/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4990 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 10/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4983 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 11/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4986 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 12/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4994 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 13/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 14/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.5009 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 15/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 16/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.4984 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 17/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6930 - auc_2: 0.4999 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 18/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4990 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 19/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 20/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.4957 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 21/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 22/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.4963 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 23/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.4999 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 24/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6929 - auc_2: 0.4975 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 25/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4997 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 26/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4991 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 27/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 28/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4991 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 29/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.5003 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 30/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.4995 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 31/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4997 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 32/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 33/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4985 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 34/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4995 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 35/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4996 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 36/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 37/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.4989 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 38/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 39/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4983 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 40/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6929 - auc_2: 0.4986 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 41/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 42/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4997 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 43/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 44/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 45/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4995 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 46/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 47/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4996 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 48/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 49/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4996 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 50/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4990 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 51/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6929 - auc_2: 0.4977 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 52/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_2: 0.4996 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 53/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 54/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4990 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 55/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4994 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 56/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.5001 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 57/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4993 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 58/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 59/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.4991 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 60/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.4985 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 61/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6929 - auc_2: 0.4985 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 62/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 63/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6929 - auc_2: 0.4977 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 64/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 65/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.5003 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 66/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4999 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 67/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4996 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 68/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6928 - auc_2: 0.5000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 69/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6929 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 70/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6929 - auc_2: 0.4987 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 71/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.5001 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 72/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6927 - auc_2: 0.4999 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 73/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4981 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 74/80\n",
      "373/373 [==============================] - 4s 12ms/step - loss: 0.6929 - auc_2: 0.4987 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 75/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4982 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 76/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4998 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 77/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4990 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 78/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6929 - auc_2: 0.4988 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 79/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6928 - auc_2: 0.4988 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 80/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6927 - auc_2: 0.4994 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
      "Epoch 1/80\n",
      "373/373 [==============================] - 6s 13ms/step - loss: 0.6818 - auc_3: 0.5819 - precision_3: 0.5497 - recall_3: 0.5630\n",
      "Epoch 2/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6636 - auc_3: 0.6376 - precision_3: 0.6014 - recall_3: 0.5727\n",
      "Epoch 3/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6625 - auc_3: 0.6392 - precision_3: 0.6023 - recall_3: 0.5546\n",
      "Epoch 4/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6590 - auc_3: 0.6473 - precision_3: 0.6012 - recall_3: 0.6129\n",
      "Epoch 5/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6495 - auc_3: 0.6674 - precision_3: 0.6169 - recall_3: 0.6025\n",
      "Epoch 6/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6459 - auc_3: 0.6738 - precision_3: 0.6267 - recall_3: 0.6049\n",
      "Epoch 7/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6446 - auc_3: 0.6754 - precision_3: 0.6269 - recall_3: 0.5891\n",
      "Epoch 8/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6393 - auc_3: 0.6849 - precision_3: 0.6323 - recall_3: 0.6062\n",
      "Epoch 9/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6347 - auc_3: 0.6921 - precision_3: 0.6405 - recall_3: 0.6027\n",
      "Epoch 10/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6334 - auc_3: 0.6943 - precision_3: 0.6416 - recall_3: 0.6031\n",
      "Epoch 11/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6294 - auc_3: 0.7003 - precision_3: 0.6411 - recall_3: 0.6249\n",
      "Epoch 12/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6300 - auc_3: 0.6991 - precision_3: 0.6394 - recall_3: 0.6215\n",
      "Epoch 13/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6268 - auc_3: 0.7037 - precision_3: 0.6438 - recall_3: 0.6243\n",
      "Epoch 14/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6235 - auc_3: 0.7090 - precision_3: 0.6493 - recall_3: 0.6339\n",
      "Epoch 15/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6225 - auc_3: 0.7104 - precision_3: 0.6527 - recall_3: 0.6246\n",
      "Epoch 16/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6205 - auc_3: 0.7127 - precision_3: 0.6533 - recall_3: 0.6232\n",
      "Epoch 17/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6228 - auc_3: 0.7093 - precision_3: 0.6465 - recall_3: 0.6308\n",
      "Epoch 18/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6207 - auc_3: 0.7125 - precision_3: 0.6506 - recall_3: 0.6321\n",
      "Epoch 19/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6189 - auc_3: 0.7148 - precision_3: 0.6522 - recall_3: 0.6394\n",
      "Epoch 20/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6174 - auc_3: 0.7169 - precision_3: 0.6513 - recall_3: 0.6368\n",
      "Epoch 21/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6155 - auc_3: 0.7197 - precision_3: 0.6556 - recall_3: 0.6409\n",
      "Epoch 22/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6135 - auc_3: 0.7222 - precision_3: 0.6560 - recall_3: 0.6465\n",
      "Epoch 23/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6137 - auc_3: 0.7217 - precision_3: 0.6596 - recall_3: 0.6394\n",
      "Epoch 24/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6148 - auc_3: 0.7202 - precision_3: 0.6563 - recall_3: 0.6417\n",
      "Epoch 25/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6128 - auc_3: 0.7232 - precision_3: 0.6586 - recall_3: 0.6448\n",
      "Epoch 26/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6112 - auc_3: 0.7253 - precision_3: 0.6587 - recall_3: 0.6467\n",
      "Epoch 27/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6117 - auc_3: 0.7242 - precision_3: 0.6612 - recall_3: 0.6398\n",
      "Epoch 28/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6105 - auc_3: 0.7260 - precision_3: 0.6610 - recall_3: 0.6439\n",
      "Epoch 29/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6092 - auc_3: 0.7278 - precision_3: 0.6628 - recall_3: 0.6419\n",
      "Epoch 30/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6091 - auc_3: 0.7277 - precision_3: 0.6627 - recall_3: 0.6433\n",
      "Epoch 31/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6097 - auc_3: 0.7270 - precision_3: 0.6589 - recall_3: 0.6539\n",
      "Epoch 32/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6089 - auc_3: 0.7281 - precision_3: 0.6605 - recall_3: 0.6503\n",
      "Epoch 33/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6069 - auc_3: 0.7303 - precision_3: 0.6654 - recall_3: 0.6447\n",
      "Epoch 34/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6075 - auc_3: 0.7297 - precision_3: 0.6648 - recall_3: 0.6457\n",
      "Epoch 35/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6064 - auc_3: 0.7309 - precision_3: 0.6620 - recall_3: 0.6536\n",
      "Epoch 36/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6067 - auc_3: 0.7308 - precision_3: 0.6644 - recall_3: 0.6547\n",
      "Epoch 37/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6046 - auc_3: 0.7332 - precision_3: 0.6672 - recall_3: 0.6451\n",
      "Epoch 38/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6054 - auc_3: 0.7327 - precision_3: 0.6664 - recall_3: 0.6542\n",
      "Epoch 39/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6055 - auc_3: 0.7322 - precision_3: 0.6655 - recall_3: 0.6506\n",
      "Epoch 40/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6037 - auc_3: 0.7342 - precision_3: 0.6674 - recall_3: 0.6426\n",
      "Epoch 41/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6030 - auc_3: 0.7350 - precision_3: 0.6686 - recall_3: 0.6446\n",
      "Epoch 42/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6017 - auc_3: 0.7367 - precision_3: 0.6685 - recall_3: 0.6492\n",
      "Epoch 43/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6033 - auc_3: 0.7347 - precision_3: 0.6662 - recall_3: 0.6520\n",
      "Epoch 44/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6003 - auc_3: 0.7386 - precision_3: 0.6704 - recall_3: 0.6501\n",
      "Epoch 45/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5997 - auc_3: 0.7392 - precision_3: 0.6727 - recall_3: 0.6536\n",
      "Epoch 46/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6007 - auc_3: 0.7381 - precision_3: 0.6692 - recall_3: 0.6555\n",
      "Epoch 47/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5996 - auc_3: 0.7391 - precision_3: 0.6713 - recall_3: 0.6462\n",
      "Epoch 48/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5998 - auc_3: 0.7390 - precision_3: 0.6717 - recall_3: 0.6537\n",
      "Epoch 49/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5974 - auc_3: 0.7421 - precision_3: 0.6754 - recall_3: 0.6484\n",
      "Epoch 50/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5982 - auc_3: 0.7407 - precision_3: 0.6731 - recall_3: 0.6463\n",
      "Epoch 51/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5966 - auc_3: 0.7427 - precision_3: 0.6744 - recall_3: 0.6563\n",
      "Epoch 52/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5967 - auc_3: 0.7428 - precision_3: 0.6744 - recall_3: 0.6586\n",
      "Epoch 53/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5968 - auc_3: 0.7426 - precision_3: 0.6743 - recall_3: 0.6524\n",
      "Epoch 54/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5964 - auc_3: 0.7430 - precision_3: 0.6709 - recall_3: 0.6582\n",
      "Epoch 55/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5962 - auc_3: 0.7428 - precision_3: 0.6766 - recall_3: 0.6456\n",
      "Epoch 56/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5948 - auc_3: 0.7452 - precision_3: 0.6713 - recall_3: 0.6642\n",
      "Epoch 57/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5931 - auc_3: 0.7470 - precision_3: 0.6777 - recall_3: 0.6560\n",
      "Epoch 58/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5934 - auc_3: 0.7464 - precision_3: 0.6776 - recall_3: 0.6533\n",
      "Epoch 59/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5906 - auc_3: 0.7497 - precision_3: 0.6776 - recall_3: 0.6680\n",
      "Epoch 60/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5914 - auc_3: 0.7484 - precision_3: 0.6810 - recall_3: 0.6509\n",
      "Epoch 61/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5917 - auc_3: 0.7482 - precision_3: 0.6767 - recall_3: 0.6608\n",
      "Epoch 62/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5913 - auc_3: 0.7485 - precision_3: 0.6761 - recall_3: 0.6576\n",
      "Epoch 63/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5909 - auc_3: 0.7491 - precision_3: 0.6794 - recall_3: 0.6572\n",
      "Epoch 64/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5899 - auc_3: 0.7501 - precision_3: 0.6791 - recall_3: 0.6619\n",
      "Epoch 65/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5881 - auc_3: 0.7522 - precision_3: 0.6788 - recall_3: 0.6608\n",
      "Epoch 66/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5885 - auc_3: 0.7519 - precision_3: 0.6834 - recall_3: 0.6568\n",
      "Epoch 67/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5875 - auc_3: 0.7526 - precision_3: 0.6802 - recall_3: 0.6614\n",
      "Epoch 68/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5869 - auc_3: 0.7534 - precision_3: 0.6810 - recall_3: 0.6655\n",
      "Epoch 69/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5868 - auc_3: 0.7533 - precision_3: 0.6816 - recall_3: 0.6594\n",
      "Epoch 70/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5856 - auc_3: 0.7545 - precision_3: 0.6832 - recall_3: 0.6553\n",
      "Epoch 71/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5838 - auc_3: 0.7564 - precision_3: 0.6817 - recall_3: 0.6707\n",
      "Epoch 72/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5840 - auc_3: 0.7563 - precision_3: 0.6830 - recall_3: 0.6617\n",
      "Epoch 73/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5839 - auc_3: 0.7566 - precision_3: 0.6815 - recall_3: 0.6717\n",
      "Epoch 74/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5824 - auc_3: 0.7582 - precision_3: 0.6830 - recall_3: 0.6696\n",
      "Epoch 75/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5823 - auc_3: 0.7581 - precision_3: 0.6848 - recall_3: 0.6654\n",
      "Epoch 76/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5830 - auc_3: 0.7568 - precision_3: 0.6807 - recall_3: 0.6727\n",
      "Epoch 77/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5815 - auc_3: 0.7588 - precision_3: 0.6842 - recall_3: 0.6739\n",
      "Epoch 78/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5810 - auc_3: 0.7594 - precision_3: 0.6847 - recall_3: 0.6687\n",
      "Epoch 79/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5814 - auc_3: 0.7590 - precision_3: 0.6821 - recall_3: 0.6727\n",
      "Epoch 80/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5799 - auc_3: 0.7607 - precision_3: 0.6878 - recall_3: 0.6694\n",
      "Epoch 1/80\n",
      "373/373 [==============================] - 6s 13ms/step - loss: 0.6784 - auc_4: 0.5907 - precision_4: 0.5614 - recall_4: 0.4708\n",
      "Epoch 2/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6524 - auc_4: 0.6612 - precision_4: 0.6220 - recall_4: 0.5513\n",
      "Epoch 3/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6466 - auc_4: 0.6719 - precision_4: 0.6285 - recall_4: 0.5727\n",
      "Epoch 4/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6410 - auc_4: 0.6816 - precision_4: 0.6276 - recall_4: 0.6095\n",
      "Epoch 5/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6361 - auc_4: 0.6897 - precision_4: 0.6336 - recall_4: 0.6183\n",
      "Epoch 6/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6324 - auc_4: 0.6960 - precision_4: 0.6426 - recall_4: 0.6147\n",
      "Epoch 7/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6312 - auc_4: 0.6974 - precision_4: 0.6415 - recall_4: 0.6270\n",
      "Epoch 8/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6298 - auc_4: 0.6999 - precision_4: 0.6425 - recall_4: 0.6282\n",
      "Epoch 9/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6245 - auc_4: 0.7068 - precision_4: 0.6434 - recall_4: 0.6220\n",
      "Epoch 10/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6239 - auc_4: 0.7084 - precision_4: 0.6478 - recall_4: 0.6349\n",
      "Epoch 11/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6238 - auc_4: 0.7082 - precision_4: 0.6455 - recall_4: 0.6357\n",
      "Epoch 12/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6214 - auc_4: 0.7114 - precision_4: 0.6478 - recall_4: 0.6316\n",
      "Epoch 13/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6198 - auc_4: 0.7136 - precision_4: 0.6486 - recall_4: 0.6400\n",
      "Epoch 14/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6183 - auc_4: 0.7158 - precision_4: 0.6504 - recall_4: 0.6502\n",
      "Epoch 15/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6184 - auc_4: 0.7156 - precision_4: 0.6492 - recall_4: 0.6494\n",
      "Epoch 16/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6186 - auc_4: 0.7151 - precision_4: 0.6500 - recall_4: 0.6483\n",
      "Epoch 17/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6170 - auc_4: 0.7177 - precision_4: 0.6530 - recall_4: 0.6406\n",
      "Epoch 18/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6136 - auc_4: 0.7221 - precision_4: 0.6553 - recall_4: 0.6415\n",
      "Epoch 19/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6153 - auc_4: 0.7196 - precision_4: 0.6518 - recall_4: 0.6517\n",
      "Epoch 20/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6133 - auc_4: 0.7225 - precision_4: 0.6519 - recall_4: 0.6568\n",
      "Epoch 21/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6113 - auc_4: 0.7249 - precision_4: 0.6584 - recall_4: 0.6541\n",
      "Epoch 22/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6117 - auc_4: 0.7241 - precision_4: 0.6556 - recall_4: 0.6493\n",
      "Epoch 23/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6116 - auc_4: 0.7248 - precision_4: 0.6563 - recall_4: 0.6562\n",
      "Epoch 24/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6101 - auc_4: 0.7269 - precision_4: 0.6578 - recall_4: 0.6544\n",
      "Epoch 25/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6101 - auc_4: 0.7261 - precision_4: 0.6582 - recall_4: 0.6530\n",
      "Epoch 26/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6088 - auc_4: 0.7279 - precision_4: 0.6640 - recall_4: 0.6445\n",
      "Epoch 27/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6088 - auc_4: 0.7280 - precision_4: 0.6599 - recall_4: 0.6501\n",
      "Epoch 28/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6079 - auc_4: 0.7290 - precision_4: 0.6604 - recall_4: 0.6551\n",
      "Epoch 29/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6061 - auc_4: 0.7313 - precision_4: 0.6623 - recall_4: 0.6569\n",
      "Epoch 30/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6053 - auc_4: 0.7321 - precision_4: 0.6630 - recall_4: 0.6511\n",
      "Epoch 31/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6065 - auc_4: 0.7310 - precision_4: 0.6617 - recall_4: 0.6539\n",
      "Epoch 32/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6051 - auc_4: 0.7326 - precision_4: 0.6631 - recall_4: 0.6580\n",
      "Epoch 33/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6058 - auc_4: 0.7318 - precision_4: 0.6633 - recall_4: 0.6561\n",
      "Epoch 34/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6048 - auc_4: 0.7332 - precision_4: 0.6615 - recall_4: 0.6646\n",
      "Epoch 35/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6025 - auc_4: 0.7363 - precision_4: 0.6658 - recall_4: 0.6637\n",
      "Epoch 36/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.6014 - auc_4: 0.7371 - precision_4: 0.6689 - recall_4: 0.6559\n",
      "Epoch 37/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6018 - auc_4: 0.7366 - precision_4: 0.6692 - recall_4: 0.6452\n",
      "Epoch 38/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6003 - auc_4: 0.7387 - precision_4: 0.6703 - recall_4: 0.6557\n",
      "Epoch 39/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6004 - auc_4: 0.7382 - precision_4: 0.6705 - recall_4: 0.6497\n",
      "Epoch 40/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6010 - auc_4: 0.7375 - precision_4: 0.6694 - recall_4: 0.6527\n",
      "Epoch 41/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6002 - auc_4: 0.7383 - precision_4: 0.6688 - recall_4: 0.6607\n",
      "Epoch 42/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5982 - auc_4: 0.7407 - precision_4: 0.6705 - recall_4: 0.6596\n",
      "Epoch 43/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5992 - auc_4: 0.7398 - precision_4: 0.6684 - recall_4: 0.6670\n",
      "Epoch 44/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5974 - auc_4: 0.7419 - precision_4: 0.6697 - recall_4: 0.6596\n",
      "Epoch 45/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5979 - auc_4: 0.7413 - precision_4: 0.6723 - recall_4: 0.6611\n",
      "Epoch 46/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5969 - auc_4: 0.7428 - precision_4: 0.6719 - recall_4: 0.6601\n",
      "Epoch 47/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5969 - auc_4: 0.7424 - precision_4: 0.6703 - recall_4: 0.6644\n",
      "Epoch 48/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5949 - auc_4: 0.7442 - precision_4: 0.6723 - recall_4: 0.6668\n",
      "Epoch 49/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5952 - auc_4: 0.7445 - precision_4: 0.6724 - recall_4: 0.6699\n",
      "Epoch 50/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5940 - auc_4: 0.7457 - precision_4: 0.6764 - recall_4: 0.6592\n",
      "Epoch 51/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5937 - auc_4: 0.7462 - precision_4: 0.6761 - recall_4: 0.6651\n",
      "Epoch 52/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5935 - auc_4: 0.7462 - precision_4: 0.6758 - recall_4: 0.6618\n",
      "Epoch 53/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5932 - auc_4: 0.7465 - precision_4: 0.6752 - recall_4: 0.6610\n",
      "Epoch 54/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5915 - auc_4: 0.7490 - precision_4: 0.6774 - recall_4: 0.6734\n",
      "Epoch 55/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5909 - auc_4: 0.7490 - precision_4: 0.6803 - recall_4: 0.6578\n",
      "Epoch 56/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5914 - auc_4: 0.7480 - precision_4: 0.6767 - recall_4: 0.6616\n",
      "Epoch 57/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5897 - auc_4: 0.7503 - precision_4: 0.6774 - recall_4: 0.6689\n",
      "Epoch 58/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5892 - auc_4: 0.7513 - precision_4: 0.6819 - recall_4: 0.6623\n",
      "Epoch 59/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5876 - auc_4: 0.7524 - precision_4: 0.6803 - recall_4: 0.6680\n",
      "Epoch 60/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5883 - auc_4: 0.7518 - precision_4: 0.6786 - recall_4: 0.6670\n",
      "Epoch 61/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5870 - auc_4: 0.7532 - precision_4: 0.6797 - recall_4: 0.6698\n",
      "Epoch 62/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5866 - auc_4: 0.7543 - precision_4: 0.6819 - recall_4: 0.6683\n",
      "Epoch 63/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5851 - auc_4: 0.7549 - precision_4: 0.6814 - recall_4: 0.6642\n",
      "Epoch 64/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5865 - auc_4: 0.7538 - precision_4: 0.6816 - recall_4: 0.6644\n",
      "Epoch 65/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5838 - auc_4: 0.7565 - precision_4: 0.6847 - recall_4: 0.6705\n",
      "Epoch 66/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5862 - auc_4: 0.7541 - precision_4: 0.6807 - recall_4: 0.6682\n",
      "Epoch 67/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5848 - auc_4: 0.7554 - precision_4: 0.6836 - recall_4: 0.6720\n",
      "Epoch 68/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5811 - auc_4: 0.7592 - precision_4: 0.6882 - recall_4: 0.6625\n",
      "Epoch 69/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5829 - auc_4: 0.7575 - precision_4: 0.6850 - recall_4: 0.6688\n",
      "Epoch 70/80\n",
      "373/373 [==============================] - 5s 12ms/step - loss: 0.5819 - auc_4: 0.7584 - precision_4: 0.6896 - recall_4: 0.6572\n",
      "Epoch 71/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5829 - auc_4: 0.7569 - precision_4: 0.6882 - recall_4: 0.6560\n",
      "Epoch 72/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5802 - auc_4: 0.7607 - precision_4: 0.6921 - recall_4: 0.6661\n",
      "Epoch 73/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5813 - auc_4: 0.7588 - precision_4: 0.6880 - recall_4: 0.6602\n",
      "Epoch 74/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5793 - auc_4: 0.7614 - precision_4: 0.6881 - recall_4: 0.6691\n",
      "Epoch 75/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5772 - auc_4: 0.7633 - precision_4: 0.6918 - recall_4: 0.6617\n",
      "Epoch 76/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5764 - auc_4: 0.7642 - precision_4: 0.6942 - recall_4: 0.6657\n",
      "Epoch 77/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5761 - auc_4: 0.7644 - precision_4: 0.6952 - recall_4: 0.6667\n",
      "Epoch 78/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5761 - auc_4: 0.7646 - precision_4: 0.6932 - recall_4: 0.6725\n",
      "Epoch 79/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5759 - auc_4: 0.7642 - precision_4: 0.6921 - recall_4: 0.6679\n",
      "Epoch 80/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.5757 - auc_4: 0.7651 - precision_4: 0.6929 - recall_4: 0.6693\n",
      "Epoch 1/80\n",
      "373/373 [==============================] - 6s 13ms/step - loss: 0.6928 - auc_5: 0.5027 - precision_5: 0.4692 - recall_5: 0.0567\n",
      "Epoch 2/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5004 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 3/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5008 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 4/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.4985 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 5/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4985 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 6/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.5008 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 7/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4969 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 8/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6930 - auc_5: 0.4987 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 9/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4981 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 10/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4996 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 11/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 12/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4973 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 13/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4996 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 14/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4992 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 15/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 16/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4999 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 17/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4991 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 18/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5001 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 19/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5006 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 20/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4993 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 21/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.5007 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 22/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4983 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 23/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4986 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 24/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4994 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 25/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 26/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4985 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 27/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.5015 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 28/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 29/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5005 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 30/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4991 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 31/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5001 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 32/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4974 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 33/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5001 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 34/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 35/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5017 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 36/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4992 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 37/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.4990 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 38/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4998 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 39/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4999 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 40/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4998 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 41/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4998 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 42/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 43/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4985 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 44/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4997 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 45/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4982 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 46/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4998 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 47/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6926 - auc_5: 0.4999 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 48/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4993 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 49/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.4993 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 50/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4991 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 51/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 52/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4995 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 53/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5004 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 54/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5005 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 55/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 56/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 57/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 58/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4990 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 59/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6926 - auc_5: 0.5001 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 60/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4978 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 61/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 62/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4984 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 63/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4989 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 64/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4997 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 65/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4995 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 66/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4996 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 67/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5001 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 68/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4992 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 69/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4985 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 70/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4994 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 71/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.5001 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 72/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.5001 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 73/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4985 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 74/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.4994 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 75/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4994 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 76/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.4988 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 77/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6929 - auc_5: 0.4973 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 78/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6927 - auc_5: 0.4991 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 79/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5003 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n",
      "Epoch 80/80\n",
      "373/373 [==============================] - 5s 13ms/step - loss: 0.6928 - auc_5: 0.5000 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking model score: 0.768\n"
     ]
    }
   ],
   "source": [
    "XG_clf = XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=1000,\n",
    "                    max_depth=9,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample=0.9,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_alpha = 0.01,\n",
    "                    tree_method = \"gpu_hist\",\n",
    "                    objective='multi:softprob',\n",
    "                    num_class=2,\n",
    "                    seed=27)\n",
    "\n",
    "XG_clf._estimator_type = \"classifier\"\n",
    "\n",
    "NN_clf = KerasClassifier(build_fn=build_model, \n",
    "                          batch_size = 500,\n",
    "                          epochs = 80)\n",
    "NN_clf._estimator_type = \"classifier\"\n",
    "\n",
    "estimators= [('XG', XG_clf), ('NN', NN_clf) ]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), stack_method='predict_proba')\n",
    "\n",
    "clf.fit(X_train_trf1, y_train)\n",
    "print(\"Stacking model score: %.3f\" % clf.score(X_test_trf1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4jX5-X1xXXj",
    "outputId": "a994be36-8423-43b4-94b8-ef28442d9c32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50729922, 0.49270078],\n",
       "       [0.1136179 , 0.8863821 ],\n",
       "       [0.10245796, 0.89754204],\n",
       "       ...,\n",
       "       [0.1093007 , 0.8906993 ],\n",
       "       [0.15384866, 0.84615134],\n",
       "       [0.29662749, 0.70337251]])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf.predict_proba(X_test_trf1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbmRm4rxWAeh"
   },
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvlcXJJSWEdH"
   },
   "source": [
    "**Hit Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HXS6--Leb_Eo"
   },
   "outputs": [],
   "source": [
    "def hit_ratio(k, extracted_cols, y_test_ori, predictions):\n",
    "\n",
    "  X_test_complete = extracted_cols\n",
    "  X_test_complete['prediction'] = predictions\n",
    "  d = dict(tuple(X_test_complete.groupby(['userId_ori'])))\n",
    "\n",
    "  Y_test_complete = extracted_cols\n",
    "  Y_test_complete['actual_rating'] = y_test_ori\n",
    "  d_true = dict(tuple(Y_test_complete.groupby(['userId_ori'])))\n",
    "\n",
    "  ratio = []\n",
    "  for userId in d:\n",
    "    topk_True = d_true[userId].sort_values(['actual_rating'], ascending = False)[:k]['movieId_ori'].values.tolist()\n",
    "    topk_pred = d[userId].sort_values(['prediction'], ascending = False)[:k]['movieId_ori'].values.tolist()\n",
    "    ratio.append(len([x for x in topk_pred if x in topk_True])/k)\n",
    "  \n",
    "  #return mean hit ratio\n",
    "  return pd.Series(ratio).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxCeas5oe8i3",
    "outputId": "57fd86f1-4a6f-4f4f-cd08-aa7614568b1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio @1 is 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio @5 is 0.03723404255319151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio @10 is 0.07765957446808502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio @20 is 0.13962765957446815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio @50 is 0.3032978723404254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio @100 is 0.5021276595744679\n"
     ]
    }
   ],
   "source": [
    "#input values of k\n",
    "k = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "for i in k:\n",
    "  print('Hit Ratio @'+ str(i) +' is ' + str(hit_ratio(i, extracted_cols, y_test_ori, clf.predict_proba(training_data['X_test'])[:, 1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-lvTeYjWNZh"
   },
   "source": [
    "**NDCG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YTvlapZdfLFH"
   },
   "outputs": [],
   "source": [
    "def discountedCumulativeGain(result):\n",
    "  dcg = []\n",
    "  for idx, val in enumerate(result): \n",
    "      numerator = val\n",
    "      # add 2 because python 0-index\n",
    "      denominator =  np.log2(idx + 2) \n",
    "      score = numerator/denominator\n",
    "      dcg.append(score)\n",
    "  return sum(dcg)\n",
    "\n",
    "def normalizedDiscountedCumulativeGain(k, result): \n",
    "  sorted_result = []\n",
    "  for i in range(0, k):\n",
    "    sorted_result.append(1)\n",
    "  dcg = discountedCumulativeGain(result)\n",
    "  idcg = discountedCumulativeGain(sorted_result)\n",
    "  ndcg = dcg / idcg\n",
    "  return ndcg\n",
    "\n",
    "def overallNDCG(k, extracted_cols, y_test_ori, predictions):\n",
    "  X_test_complete = extracted_cols\n",
    "  X_test_complete['prediction'] = predictions\n",
    "  d = dict(tuple(X_test_complete.groupby(['userId_ori'])))\n",
    "\n",
    "  Y_test_complete = extracted_cols\n",
    "  Y_test_complete['actual_rating'] = y_test_ori\n",
    "  d_true = dict(tuple(Y_test_complete.groupby(['userId_ori']))) \n",
    "\n",
    "  ndcg_lst = []\n",
    "  for userId in d:\n",
    "    topk_True = d_true[userId].sort_values(['actual_rating'], ascending = False)[:k]['movieId_ori'].values.tolist()\n",
    "    topk_pred = d[userId].sort_values(['prediction'], ascending = False)[:k]['movieId_ori'].values.tolist()\n",
    "    result = []\n",
    "    for i in range(0, len(topk_pred)):\n",
    "      if topk_pred[i] in topk_True:\n",
    "        result.append(1)\n",
    "      else:\n",
    "        result.append(0)\n",
    "    \n",
    "    ndcg_lst.append(normalizedDiscountedCumulativeGain(k, result))\n",
    "\n",
    "  return pd.Series(ndcg_lst).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vw9f11kfRbF",
    "outputId": "9c4733e9-577d-4809-c926-fb0a3dd367bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @1 is 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @5 is 0.03661422627619494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @10 is 0.07803478339236052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @20 is 0.1431118432930474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @50 is 0.31651288737920746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @100 is 0.5225383477414228\n"
     ]
    }
   ],
   "source": [
    "#input values of k\n",
    "k = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "for i in k:\n",
    "  print('NDCG @'+ str(i) +' is ' + str(overallNDCG(i, extracted_cols, y_test_ori, clf.predict_proba(training_data['X_test'])[:, 1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkhQUqsUWQ0n"
   },
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGfgpkj6fg1K",
    "outputId": "60a2e441-b302-4823-e6d1-62acc521de21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "output = extracted_cols\n",
    "predictions = clf.predict_proba(training_data['X_test'])[:, 1:]\n",
    "output['prediction'] = predictions\n",
    "d = dict(tuple(output.groupby(['userId_ori'])))\n",
    "\n",
    "def recommendTop10(userId):\n",
    "  return d[userId].sort_values(['prediction'], ascending = False)['primaryTitle'][:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmVwCAalfob-",
    "outputId": "caa33535-aefa-4b04-ca44-2f0bc842fcda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         Braveheart\n",
       "1                       Forrest Gump\n",
       "2                   Some Like It Hot\n",
       "3                          Pinocchio\n",
       "4    Monty Python and the Holy Grail\n",
       "5                              Split\n",
       "6                           Sideways\n",
       "7                            Amadeus\n",
       "8                           Superman\n",
       "9                          Gladiator\n",
       "Name: primaryTitle, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 recommendations for user 1920\n",
    "recommendTop10(1920)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "newFinal Stacking 2bins.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
